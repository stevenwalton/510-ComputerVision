{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision - UOCIS-510 - Homework 1\n",
    "__Part 1: (4/15 pts)__\n",
    "\n",
    "Implement Nearest Neighbor classifier for CIFAR10 dataset.\n",
    "\n",
    "Report the accuracy on test set for both L1 and L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler as SRS\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "k = 1 # Amount of nearest neighbors\n",
    "bs = 1 # Batch Size (DON'T CHANGE FROM 1)\n",
    "numworkers = 1 # number of workers\n",
    "testPercent = 0.1 # % of dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using 5000 images\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "])\n",
    "cifar10 = torchvision.datasets.CIFAR10(\"../datasets\",train=True,download=True, transform=transform)\n",
    "\n",
    "indices = np.arange(len(cifar10))\n",
    "np.random.shuffle(indices)\n",
    "size = int(testPercent*len(cifar10))\n",
    "ind = indices[:size]\n",
    "print(f\"Using {size} images\")\n",
    "\n",
    "sampler = SRS(ind)\n",
    "\n",
    "dataSet = DataLoader(cifar10,\n",
    "                     batch_size=bs,\n",
    "                     sampler=sampler,\n",
    "                     shuffle=False,\n",
    "                     num_workers=numworkers,\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPDist(image, dataset, k=1, lp=2):\n",
    "    '''\n",
    "    Computes the Lp loss between a dataset\n",
    "    '''\n",
    "    #dist = torch.tensor(len(dataset))\n",
    "    dist = torch.FloatTensor(\n",
    "        [torch.pow(\n",
    "            torch.sum(\n",
    "                torch.pow(\n",
    "                    torch.sub(image,dataset[i][0]), # sub\n",
    "                    lp)# pow\n",
    "                    ),# sum\n",
    "            1/lp) # pow \n",
    "         for i in range(len(dataset))])\n",
    "    #return dist, torch.argmin(dist)\n",
    "    return dist, torch.argsort(dist)[1:k+1] # The closest is the input itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Closest(mins, dataset):\n",
    "    '''\n",
    "    Uses a single pass by storing in a hashtable instead of looping twice\n",
    "    '''\n",
    "    dict = {}\n",
    "    for m in mins:\n",
    "        k = dataset[m][1]\n",
    "        if k in dict.keys():\n",
    "            dict[k] += 1\n",
    "        else:\n",
    "            dict[k] = 1\n",
    "    #print(dict)\n",
    "    max = torch.zeros(1)\n",
    "    for key in dict.keys():\n",
    "        if dict[key] > max:\n",
    "            max[0] = key\n",
    "    return max\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(true, label):\n",
    "    #print(true)\n",
    "    #print(label)\n",
    "    if true.item() == label: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 1.0\n",
      "Step 1/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.5\n",
      "Step 2/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 3/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.25\n",
      "Step 4/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.2\n",
      "Step 5/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 6/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.2857142857142857\n",
      "Step 7/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.25\n",
      "Step 8/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 9/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.3\n",
      "Step 10/5000\n",
      "LP1 Accuracy: 0.0\n",
      "LP2 Accuracy: 0.2727272727272727\n",
      "Step 11/5000\n",
      "LP1 Accuracy: 0.08333333333333333\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 12/5000\n",
      "LP1 Accuracy: 0.07692307692307693\n",
      "LP2 Accuracy: 0.38461538461538464\n",
      "Step 13/5000\n",
      "LP1 Accuracy: 0.07142857142857142\n",
      "LP2 Accuracy: 0.42857142857142855\n",
      "Step 14/5000\n",
      "LP1 Accuracy: 0.06666666666666667\n",
      "LP2 Accuracy: 0.4666666666666667\n",
      "Step 15/5000\n",
      "LP1 Accuracy: 0.0625\n",
      "LP2 Accuracy: 0.4375\n",
      "Step 16/5000\n",
      "LP1 Accuracy: 0.058823529411764705\n",
      "LP2 Accuracy: 0.47058823529411764\n",
      "Step 17/5000\n",
      "LP1 Accuracy: 0.05555555555555555\n",
      "LP2 Accuracy: 0.4444444444444444\n",
      "Step 18/5000\n",
      "LP1 Accuracy: 0.05263157894736842\n",
      "LP2 Accuracy: 0.42105263157894735\n",
      "Step 19/5000\n",
      "LP1 Accuracy: 0.05\n",
      "LP2 Accuracy: 0.45\n",
      "Step 20/5000\n",
      "LP1 Accuracy: 0.047619047619047616\n",
      "LP2 Accuracy: 0.47619047619047616\n",
      "Step 21/5000\n",
      "LP1 Accuracy: 0.045454545454545456\n",
      "LP2 Accuracy: 0.45454545454545453\n",
      "Step 22/5000\n",
      "LP1 Accuracy: 0.043478260869565216\n",
      "LP2 Accuracy: 0.43478260869565216\n",
      "Step 23/5000\n",
      "LP1 Accuracy: 0.041666666666666664\n",
      "LP2 Accuracy: 0.4583333333333333\n",
      "Step 24/5000\n",
      "LP1 Accuracy: 0.04\n",
      "LP2 Accuracy: 0.44\n",
      "Step 25/5000\n",
      "LP1 Accuracy: 0.038461538461538464\n",
      "LP2 Accuracy: 0.4230769230769231\n",
      "Step 26/5000\n",
      "LP1 Accuracy: 0.037037037037037035\n",
      "LP2 Accuracy: 0.4074074074074074\n",
      "Step 27/5000\n",
      "LP1 Accuracy: 0.03571428571428571\n",
      "LP2 Accuracy: 0.39285714285714285\n",
      "Step 28/5000\n",
      "LP1 Accuracy: 0.034482758620689655\n",
      "LP2 Accuracy: 0.41379310344827586\n",
      "Step 29/5000\n",
      "LP1 Accuracy: 0.03333333333333333\n",
      "LP2 Accuracy: 0.4\n",
      "Step 30/5000\n",
      "LP1 Accuracy: 0.03225806451612903\n",
      "LP2 Accuracy: 0.3870967741935484\n",
      "Step 31/5000\n",
      "LP1 Accuracy: 0.03125\n",
      "LP2 Accuracy: 0.375\n",
      "Step 32/5000\n",
      "LP1 Accuracy: 0.030303030303030304\n",
      "LP2 Accuracy: 0.36363636363636365\n",
      "Step 33/5000\n",
      "LP1 Accuracy: 0.029411764705882353\n",
      "LP2 Accuracy: 0.35294117647058826\n",
      "Step 34/5000\n",
      "LP1 Accuracy: 0.05714285714285714\n",
      "LP2 Accuracy: 0.34285714285714286\n",
      "Step 35/5000\n",
      "LP1 Accuracy: 0.05555555555555555\n",
      "LP2 Accuracy: 0.3611111111111111\n",
      "Step 36/5000\n",
      "LP1 Accuracy: 0.05405405405405406\n",
      "LP2 Accuracy: 0.35135135135135137\n",
      "Step 37/5000\n",
      "LP1 Accuracy: 0.05263157894736842\n",
      "LP2 Accuracy: 0.34210526315789475\n",
      "Step 38/5000\n",
      "LP1 Accuracy: 0.05128205128205128\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 39/5000\n",
      "LP1 Accuracy: 0.075\n",
      "LP2 Accuracy: 0.35\n",
      "Step 40/5000\n",
      "LP1 Accuracy: 0.07317073170731707\n",
      "LP2 Accuracy: 0.36585365853658536\n",
      "Step 41/5000\n",
      "LP1 Accuracy: 0.07142857142857142\n",
      "LP2 Accuracy: 0.38095238095238093\n",
      "Step 42/5000\n",
      "LP1 Accuracy: 0.09302325581395349\n",
      "LP2 Accuracy: 0.37209302325581395\n",
      "Step 43/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.38636363636363635\n",
      "Step 44/5000\n",
      "LP1 Accuracy: 0.08888888888888889\n",
      "LP2 Accuracy: 0.4\n",
      "Step 45/5000\n",
      "LP1 Accuracy: 0.08695652173913043\n",
      "LP2 Accuracy: 0.391304347826087\n",
      "Step 46/5000\n",
      "LP1 Accuracy: 0.0851063829787234\n",
      "LP2 Accuracy: 0.3829787234042553\n",
      "Step 47/5000\n",
      "LP1 Accuracy: 0.08333333333333333\n",
      "LP2 Accuracy: 0.375\n",
      "Step 48/5000\n",
      "LP1 Accuracy: 0.08163265306122448\n",
      "LP2 Accuracy: 0.3673469387755102\n",
      "Step 49/5000\n",
      "LP1 Accuracy: 0.08\n",
      "LP2 Accuracy: 0.36\n",
      "Step 50/5000\n",
      "LP1 Accuracy: 0.0784313725490196\n",
      "LP2 Accuracy: 0.35294117647058826\n",
      "Step 51/5000\n",
      "LP1 Accuracy: 0.07692307692307693\n",
      "LP2 Accuracy: 0.36538461538461536\n",
      "Step 52/5000\n",
      "LP1 Accuracy: 0.07547169811320754\n",
      "LP2 Accuracy: 0.3584905660377358\n",
      "Step 53/5000\n",
      "LP1 Accuracy: 0.07407407407407407\n",
      "LP2 Accuracy: 0.37037037037037035\n",
      "Step 54/5000\n",
      "LP1 Accuracy: 0.07272727272727272\n",
      "LP2 Accuracy: 0.38181818181818183\n",
      "Step 55/5000\n",
      "LP1 Accuracy: 0.07142857142857142\n",
      "LP2 Accuracy: 0.375\n",
      "Step 56/5000\n",
      "LP1 Accuracy: 0.07017543859649122\n",
      "LP2 Accuracy: 0.38596491228070173\n",
      "Step 57/5000\n",
      "LP1 Accuracy: 0.06896551724137931\n",
      "LP2 Accuracy: 0.3793103448275862\n",
      "Step 58/5000\n",
      "LP1 Accuracy: 0.06779661016949153\n",
      "LP2 Accuracy: 0.3898305084745763\n",
      "Step 59/5000\n",
      "LP1 Accuracy: 0.06666666666666667\n",
      "LP2 Accuracy: 0.38333333333333336\n",
      "Step 60/5000\n",
      "LP1 Accuracy: 0.06557377049180328\n",
      "LP2 Accuracy: 0.39344262295081966\n",
      "Step 61/5000\n",
      "LP1 Accuracy: 0.06451612903225806\n",
      "LP2 Accuracy: 0.3870967741935484\n",
      "Step 62/5000\n",
      "LP1 Accuracy: 0.06349206349206349\n",
      "LP2 Accuracy: 0.38095238095238093\n",
      "Step 63/5000\n",
      "LP1 Accuracy: 0.0625\n",
      "LP2 Accuracy: 0.375\n",
      "Step 64/5000\n",
      "LP1 Accuracy: 0.06153846153846154\n",
      "LP2 Accuracy: 0.38461538461538464\n",
      "Step 65/5000\n",
      "LP1 Accuracy: 0.06060606060606061\n",
      "LP2 Accuracy: 0.3787878787878788\n",
      "Step 66/5000\n",
      "LP1 Accuracy: 0.05970149253731343\n",
      "LP2 Accuracy: 0.373134328358209\n",
      "Step 67/5000\n",
      "LP1 Accuracy: 0.058823529411764705\n",
      "LP2 Accuracy: 0.38235294117647056\n",
      "Step 68/5000\n",
      "LP1 Accuracy: 0.057971014492753624\n",
      "LP2 Accuracy: 0.391304347826087\n",
      "Step 69/5000\n",
      "LP1 Accuracy: 0.05714285714285714\n",
      "LP2 Accuracy: 0.4\n",
      "Step 70/5000\n",
      "LP1 Accuracy: 0.056338028169014086\n",
      "LP2 Accuracy: 0.39436619718309857\n",
      "Step 71/5000\n",
      "LP1 Accuracy: 0.06944444444444445\n",
      "LP2 Accuracy: 0.4027777777777778\n",
      "Step 72/5000\n",
      "LP1 Accuracy: 0.0684931506849315\n",
      "LP2 Accuracy: 0.410958904109589\n",
      "Step 73/5000\n",
      "LP1 Accuracy: 0.06756756756756757\n",
      "LP2 Accuracy: 0.40540540540540543\n",
      "Step 74/5000\n",
      "LP1 Accuracy: 0.06666666666666667\n",
      "LP2 Accuracy: 0.4\n",
      "Step 75/5000\n",
      "LP1 Accuracy: 0.07894736842105263\n",
      "LP2 Accuracy: 0.39473684210526316\n",
      "Step 76/5000\n",
      "LP1 Accuracy: 0.07792207792207792\n",
      "LP2 Accuracy: 0.38961038961038963\n",
      "Step 77/5000\n",
      "LP1 Accuracy: 0.07692307692307693\n",
      "LP2 Accuracy: 0.38461538461538464\n",
      "Step 78/5000\n",
      "LP1 Accuracy: 0.0759493670886076\n",
      "LP2 Accuracy: 0.379746835443038\n",
      "Step 79/5000\n",
      "LP1 Accuracy: 0.075\n",
      "LP2 Accuracy: 0.375\n",
      "Step 80/5000\n",
      "LP1 Accuracy: 0.07407407407407407\n",
      "LP2 Accuracy: 0.37037037037037035\n",
      "Step 81/5000\n",
      "LP1 Accuracy: 0.07317073170731707\n",
      "LP2 Accuracy: 0.36585365853658536\n",
      "Step 82/5000\n",
      "LP1 Accuracy: 0.08433734939759036\n",
      "LP2 Accuracy: 0.3614457831325301\n",
      "Step 83/5000\n",
      "LP1 Accuracy: 0.08333333333333333\n",
      "LP2 Accuracy: 0.35714285714285715\n",
      "Step 84/5000\n",
      "LP1 Accuracy: 0.08235294117647059\n",
      "LP2 Accuracy: 0.35294117647058826\n",
      "Step 85/5000\n",
      "LP1 Accuracy: 0.08139534883720931\n",
      "LP2 Accuracy: 0.36046511627906974\n",
      "Step 86/5000\n",
      "LP1 Accuracy: 0.09195402298850575\n",
      "LP2 Accuracy: 0.3563218390804598\n",
      "Step 87/5000\n",
      "LP1 Accuracy: 0.10227272727272728\n",
      "LP2 Accuracy: 0.36363636363636365\n",
      "Step 88/5000\n",
      "LP1 Accuracy: 0.10112359550561797\n",
      "LP2 Accuracy: 0.3595505617977528\n",
      "Step 89/5000\n",
      "LP1 Accuracy: 0.1\n",
      "LP2 Accuracy: 0.35555555555555557\n",
      "Step 90/5000\n",
      "LP1 Accuracy: 0.0989010989010989\n",
      "LP2 Accuracy: 0.3516483516483517\n",
      "Step 91/5000\n",
      "LP1 Accuracy: 0.09782608695652174\n",
      "LP2 Accuracy: 0.34782608695652173\n",
      "Step 92/5000\n",
      "LP1 Accuracy: 0.0967741935483871\n",
      "LP2 Accuracy: 0.34408602150537637\n",
      "Step 93/5000\n",
      "LP1 Accuracy: 0.09574468085106383\n",
      "LP2 Accuracy: 0.3404255319148936\n",
      "Step 94/5000\n",
      "LP1 Accuracy: 0.09473684210526316\n",
      "LP2 Accuracy: 0.3473684210526316\n",
      "Step 95/5000\n",
      "LP1 Accuracy: 0.09375\n",
      "LP2 Accuracy: 0.34375\n",
      "Step 96/5000\n",
      "LP1 Accuracy: 0.10309278350515463\n",
      "LP2 Accuracy: 0.35051546391752575\n",
      "Step 97/5000\n",
      "LP1 Accuracy: 0.10204081632653061\n",
      "LP2 Accuracy: 0.3469387755102041\n",
      "Step 98/5000\n",
      "LP1 Accuracy: 0.10101010101010101\n",
      "LP2 Accuracy: 0.3434343434343434\n",
      "Step 99/5000\n",
      "LP1 Accuracy: 0.1\n",
      "LP2 Accuracy: 0.35\n",
      "Step 100/5000\n",
      "LP1 Accuracy: 0.09900990099009901\n",
      "LP2 Accuracy: 0.3465346534653465\n",
      "Step 101/5000\n",
      "LP1 Accuracy: 0.09803921568627451\n",
      "LP2 Accuracy: 0.3431372549019608\n",
      "Step 102/5000\n",
      "LP1 Accuracy: 0.0970873786407767\n",
      "LP2 Accuracy: 0.34951456310679613\n",
      "Step 103/5000\n",
      "LP1 Accuracy: 0.09615384615384616\n",
      "LP2 Accuracy: 0.34615384615384615\n",
      "Step 104/5000\n",
      "LP1 Accuracy: 0.09523809523809523\n",
      "LP2 Accuracy: 0.34285714285714286\n",
      "Step 105/5000\n",
      "LP1 Accuracy: 0.09433962264150944\n",
      "LP2 Accuracy: 0.33962264150943394\n",
      "Step 106/5000\n",
      "LP1 Accuracy: 0.09345794392523364\n",
      "LP2 Accuracy: 0.3364485981308411\n",
      "Step 107/5000\n",
      "LP1 Accuracy: 0.09259259259259259\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 108/5000\n",
      "LP1 Accuracy: 0.10091743119266056\n",
      "LP2 Accuracy: 0.3302752293577982\n",
      "Step 109/5000\n",
      "LP1 Accuracy: 0.10909090909090909\n",
      "LP2 Accuracy: 0.33636363636363636\n",
      "Step 110/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP1 Accuracy: 0.10810810810810811\n",
      "LP2 Accuracy: 0.34234234234234234\n",
      "Step 111/5000\n",
      "LP1 Accuracy: 0.10714285714285714\n",
      "LP2 Accuracy: 0.3392857142857143\n",
      "Step 112/5000\n",
      "LP1 Accuracy: 0.10619469026548672\n",
      "LP2 Accuracy: 0.336283185840708\n",
      "Step 113/5000\n",
      "LP1 Accuracy: 0.11403508771929824\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 114/5000\n",
      "LP1 Accuracy: 0.11304347826086956\n",
      "LP2 Accuracy: 0.33043478260869563\n",
      "Step 115/5000\n",
      "LP1 Accuracy: 0.11206896551724138\n",
      "LP2 Accuracy: 0.3275862068965517\n",
      "Step 116/5000\n",
      "LP1 Accuracy: 0.1111111111111111\n",
      "LP2 Accuracy: 0.3247863247863248\n",
      "Step 117/5000\n",
      "LP1 Accuracy: 0.11016949152542373\n",
      "LP2 Accuracy: 0.3305084745762712\n",
      "Step 118/5000\n",
      "LP1 Accuracy: 0.1092436974789916\n",
      "LP2 Accuracy: 0.33613445378151263\n",
      "Step 119/5000\n",
      "LP1 Accuracy: 0.10833333333333334\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 120/5000\n",
      "LP1 Accuracy: 0.10743801652892562\n",
      "LP2 Accuracy: 0.3305785123966942\n",
      "Step 121/5000\n",
      "LP1 Accuracy: 0.10655737704918032\n",
      "LP2 Accuracy: 0.32786885245901637\n",
      "Step 122/5000\n",
      "LP1 Accuracy: 0.10569105691056911\n",
      "LP2 Accuracy: 0.3252032520325203\n",
      "Step 123/5000\n",
      "LP1 Accuracy: 0.10483870967741936\n",
      "LP2 Accuracy: 0.33064516129032256\n",
      "Step 124/5000\n",
      "LP1 Accuracy: 0.104\n",
      "LP2 Accuracy: 0.328\n",
      "Step 125/5000\n",
      "LP1 Accuracy: 0.10317460317460317\n",
      "LP2 Accuracy: 0.3253968253968254\n",
      "Step 126/5000\n",
      "LP1 Accuracy: 0.10236220472440945\n",
      "LP2 Accuracy: 0.3228346456692913\n",
      "Step 127/5000\n",
      "LP1 Accuracy: 0.1015625\n",
      "LP2 Accuracy: 0.3203125\n",
      "Step 128/5000\n",
      "LP1 Accuracy: 0.10077519379844961\n",
      "LP2 Accuracy: 0.3178294573643411\n",
      "Step 129/5000\n",
      "LP1 Accuracy: 0.1076923076923077\n",
      "LP2 Accuracy: 0.3230769230769231\n",
      "Step 130/5000\n",
      "LP1 Accuracy: 0.10687022900763359\n",
      "LP2 Accuracy: 0.3282442748091603\n",
      "Step 131/5000\n",
      "LP1 Accuracy: 0.10606060606060606\n",
      "LP2 Accuracy: 0.32575757575757575\n",
      "Step 132/5000\n",
      "LP1 Accuracy: 0.10526315789473684\n",
      "LP2 Accuracy: 0.3233082706766917\n",
      "Step 133/5000\n",
      "LP1 Accuracy: 0.1044776119402985\n",
      "LP2 Accuracy: 0.3283582089552239\n",
      "Step 134/5000\n",
      "LP1 Accuracy: 0.1037037037037037\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 135/5000\n",
      "LP1 Accuracy: 0.10294117647058823\n",
      "LP2 Accuracy: 0.33088235294117646\n",
      "Step 136/5000\n",
      "LP1 Accuracy: 0.10218978102189781\n",
      "LP2 Accuracy: 0.3284671532846715\n",
      "Step 137/5000\n",
      "LP1 Accuracy: 0.10144927536231885\n",
      "LP2 Accuracy: 0.32608695652173914\n",
      "Step 138/5000\n",
      "LP1 Accuracy: 0.10071942446043165\n",
      "LP2 Accuracy: 0.33093525179856115\n",
      "Step 139/5000\n",
      "LP1 Accuracy: 0.1\n",
      "LP2 Accuracy: 0.32857142857142857\n",
      "Step 140/5000\n",
      "LP1 Accuracy: 0.09929078014184398\n",
      "LP2 Accuracy: 0.3262411347517731\n",
      "Step 141/5000\n",
      "LP1 Accuracy: 0.09859154929577464\n",
      "LP2 Accuracy: 0.323943661971831\n",
      "Step 142/5000\n",
      "LP1 Accuracy: 0.0979020979020979\n",
      "LP2 Accuracy: 0.32167832167832167\n",
      "Step 143/5000\n",
      "LP1 Accuracy: 0.09722222222222222\n",
      "LP2 Accuracy: 0.3263888888888889\n",
      "Step 144/5000\n",
      "LP1 Accuracy: 0.09655172413793103\n",
      "LP2 Accuracy: 0.3310344827586207\n",
      "Step 145/5000\n",
      "LP1 Accuracy: 0.0958904109589041\n",
      "LP2 Accuracy: 0.3356164383561644\n",
      "Step 146/5000\n",
      "LP1 Accuracy: 0.09523809523809523\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 147/5000\n",
      "LP1 Accuracy: 0.0945945945945946\n",
      "LP2 Accuracy: 0.33783783783783783\n",
      "Step 148/5000\n",
      "LP1 Accuracy: 0.09395973154362416\n",
      "LP2 Accuracy: 0.33557046979865773\n",
      "Step 149/5000\n",
      "LP1 Accuracy: 0.09333333333333334\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 150/5000\n",
      "LP1 Accuracy: 0.09933774834437085\n",
      "LP2 Accuracy: 0.33774834437086093\n",
      "Step 151/5000\n",
      "LP1 Accuracy: 0.09868421052631579\n",
      "LP2 Accuracy: 0.3355263157894737\n",
      "Step 152/5000\n",
      "LP1 Accuracy: 0.09803921568627451\n",
      "LP2 Accuracy: 0.33986928104575165\n",
      "Step 153/5000\n",
      "LP1 Accuracy: 0.09740259740259741\n",
      "LP2 Accuracy: 0.33766233766233766\n",
      "Step 154/5000\n",
      "LP1 Accuracy: 0.0967741935483871\n",
      "LP2 Accuracy: 0.3419354838709677\n",
      "Step 155/5000\n",
      "LP1 Accuracy: 0.09615384615384616\n",
      "LP2 Accuracy: 0.33974358974358976\n",
      "Step 156/5000\n",
      "LP1 Accuracy: 0.10191082802547771\n",
      "LP2 Accuracy: 0.3375796178343949\n",
      "Step 157/5000\n",
      "LP1 Accuracy: 0.10126582278481013\n",
      "LP2 Accuracy: 0.33544303797468356\n",
      "Step 158/5000\n",
      "LP1 Accuracy: 0.10062893081761007\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 159/5000\n",
      "LP1 Accuracy: 0.1\n",
      "LP2 Accuracy: 0.3375\n",
      "Step 160/5000\n",
      "LP1 Accuracy: 0.09937888198757763\n",
      "LP2 Accuracy: 0.33540372670807456\n",
      "Step 161/5000\n",
      "LP1 Accuracy: 0.09876543209876543\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 162/5000\n",
      "LP1 Accuracy: 0.09815950920245399\n",
      "LP2 Accuracy: 0.3312883435582822\n",
      "Step 163/5000\n",
      "LP1 Accuracy: 0.0975609756097561\n",
      "LP2 Accuracy: 0.32926829268292684\n",
      "Step 164/5000\n",
      "LP1 Accuracy: 0.10303030303030303\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 165/5000\n",
      "LP1 Accuracy: 0.10240963855421686\n",
      "LP2 Accuracy: 0.3313253012048193\n",
      "Step 166/5000\n",
      "LP1 Accuracy: 0.10778443113772455\n",
      "LP2 Accuracy: 0.32934131736526945\n",
      "Step 167/5000\n",
      "LP1 Accuracy: 0.10714285714285714\n",
      "LP2 Accuracy: 0.3273809523809524\n",
      "Step 168/5000\n",
      "LP1 Accuracy: 0.10650887573964497\n",
      "LP2 Accuracy: 0.3254437869822485\n",
      "Step 169/5000\n",
      "LP1 Accuracy: 0.10588235294117647\n",
      "LP2 Accuracy: 0.32941176470588235\n",
      "Step 170/5000\n",
      "LP1 Accuracy: 0.10526315789473684\n",
      "LP2 Accuracy: 0.32748538011695905\n",
      "Step 171/5000\n",
      "LP1 Accuracy: 0.10465116279069768\n",
      "LP2 Accuracy: 0.3313953488372093\n",
      "Step 172/5000\n",
      "LP1 Accuracy: 0.10404624277456648\n",
      "LP2 Accuracy: 0.32947976878612717\n",
      "Step 173/5000\n",
      "LP1 Accuracy: 0.10344827586206896\n",
      "LP2 Accuracy: 0.3275862068965517\n",
      "Step 174/5000\n",
      "LP1 Accuracy: 0.10285714285714286\n",
      "LP2 Accuracy: 0.32571428571428573\n",
      "Step 175/5000\n",
      "LP1 Accuracy: 0.10227272727272728\n",
      "LP2 Accuracy: 0.32386363636363635\n",
      "Step 176/5000\n",
      "LP1 Accuracy: 0.1016949152542373\n",
      "LP2 Accuracy: 0.327683615819209\n",
      "Step 177/5000\n",
      "LP1 Accuracy: 0.10112359550561797\n",
      "LP2 Accuracy: 0.3258426966292135\n",
      "Step 178/5000\n",
      "LP1 Accuracy: 0.1005586592178771\n",
      "LP2 Accuracy: 0.329608938547486\n",
      "Step 179/5000\n",
      "LP1 Accuracy: 0.1\n",
      "LP2 Accuracy: 0.3277777777777778\n",
      "Step 180/5000\n",
      "LP1 Accuracy: 0.09944751381215469\n",
      "LP2 Accuracy: 0.3314917127071823\n",
      "Step 181/5000\n",
      "LP1 Accuracy: 0.0989010989010989\n",
      "LP2 Accuracy: 0.32967032967032966\n",
      "Step 182/5000\n",
      "LP1 Accuracy: 0.09836065573770492\n",
      "LP2 Accuracy: 0.32786885245901637\n",
      "Step 183/5000\n",
      "LP1 Accuracy: 0.09782608695652174\n",
      "LP2 Accuracy: 0.32608695652173914\n",
      "Step 184/5000\n",
      "LP1 Accuracy: 0.0972972972972973\n",
      "LP2 Accuracy: 0.32432432432432434\n",
      "Step 185/5000\n",
      "LP1 Accuracy: 0.0967741935483871\n",
      "LP2 Accuracy: 0.3279569892473118\n",
      "Step 186/5000\n",
      "LP1 Accuracy: 0.0962566844919786\n",
      "LP2 Accuracy: 0.32620320855614976\n",
      "Step 187/5000\n",
      "LP1 Accuracy: 0.09574468085106383\n",
      "LP2 Accuracy: 0.324468085106383\n",
      "Step 188/5000\n",
      "LP1 Accuracy: 0.09523809523809523\n",
      "LP2 Accuracy: 0.32275132275132273\n",
      "Step 189/5000\n",
      "LP1 Accuracy: 0.09473684210526316\n",
      "LP2 Accuracy: 0.3263157894736842\n",
      "Step 190/5000\n",
      "LP1 Accuracy: 0.09424083769633508\n",
      "LP2 Accuracy: 0.3298429319371728\n",
      "Step 191/5000\n",
      "LP1 Accuracy: 0.09375\n",
      "LP2 Accuracy: 0.328125\n",
      "Step 192/5000\n",
      "LP1 Accuracy: 0.09326424870466321\n",
      "LP2 Accuracy: 0.32642487046632124\n",
      "Step 193/5000\n",
      "LP1 Accuracy: 0.09278350515463918\n",
      "LP2 Accuracy: 0.3247422680412371\n",
      "Step 194/5000\n",
      "LP1 Accuracy: 0.09230769230769231\n",
      "LP2 Accuracy: 0.3230769230769231\n",
      "Step 195/5000\n",
      "LP1 Accuracy: 0.09183673469387756\n",
      "LP2 Accuracy: 0.32142857142857145\n",
      "Step 196/5000\n",
      "LP1 Accuracy: 0.09137055837563451\n",
      "LP2 Accuracy: 0.3197969543147208\n",
      "Step 197/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.3181818181818182\n",
      "Step 198/5000\n",
      "LP1 Accuracy: 0.09045226130653267\n",
      "LP2 Accuracy: 0.3165829145728643\n",
      "Step 199/5000\n",
      "LP1 Accuracy: 0.09\n",
      "LP2 Accuracy: 0.315\n",
      "Step 200/5000\n",
      "LP1 Accuracy: 0.08955223880597014\n",
      "LP2 Accuracy: 0.31343283582089554\n",
      "Step 201/5000\n",
      "LP1 Accuracy: 0.09405940594059406\n",
      "LP2 Accuracy: 0.3118811881188119\n",
      "Step 202/5000\n",
      "LP1 Accuracy: 0.09359605911330049\n",
      "LP2 Accuracy: 0.31527093596059114\n",
      "Step 203/5000\n",
      "LP1 Accuracy: 0.09313725490196079\n",
      "LP2 Accuracy: 0.3137254901960784\n",
      "Step 204/5000\n",
      "LP1 Accuracy: 0.09268292682926829\n",
      "LP2 Accuracy: 0.3170731707317073\n",
      "Step 205/5000\n",
      "LP1 Accuracy: 0.09223300970873786\n",
      "LP2 Accuracy: 0.3155339805825243\n",
      "Step 206/5000\n",
      "LP1 Accuracy: 0.09178743961352658\n",
      "LP2 Accuracy: 0.3140096618357488\n",
      "Step 207/5000\n",
      "LP1 Accuracy: 0.09134615384615384\n",
      "LP2 Accuracy: 0.3125\n",
      "Step 208/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.31100478468899523\n",
      "Step 209/5000\n",
      "LP1 Accuracy: 0.09047619047619047\n",
      "LP2 Accuracy: 0.3142857142857143\n",
      "Step 210/5000\n",
      "LP1 Accuracy: 0.09004739336492891\n",
      "LP2 Accuracy: 0.3175355450236967\n",
      "Step 211/5000\n",
      "LP1 Accuracy: 0.08962264150943396\n",
      "LP2 Accuracy: 0.3160377358490566\n",
      "Step 212/5000\n",
      "LP1 Accuracy: 0.0892018779342723\n",
      "LP2 Accuracy: 0.3145539906103286\n",
      "Step 213/5000\n",
      "LP1 Accuracy: 0.08878504672897196\n",
      "LP2 Accuracy: 0.3177570093457944\n",
      "Step 214/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP1 Accuracy: 0.08837209302325581\n",
      "LP2 Accuracy: 0.31627906976744186\n",
      "Step 215/5000\n",
      "LP1 Accuracy: 0.08796296296296297\n",
      "LP2 Accuracy: 0.3148148148148148\n",
      "Step 216/5000\n",
      "LP1 Accuracy: 0.08755760368663594\n",
      "LP2 Accuracy: 0.31797235023041476\n",
      "Step 217/5000\n",
      "LP1 Accuracy: 0.0871559633027523\n",
      "LP2 Accuracy: 0.3165137614678899\n",
      "Step 218/5000\n",
      "LP1 Accuracy: 0.0867579908675799\n",
      "LP2 Accuracy: 0.3150684931506849\n",
      "Step 219/5000\n",
      "LP1 Accuracy: 0.08636363636363636\n",
      "LP2 Accuracy: 0.3181818181818182\n",
      "Step 220/5000\n",
      "LP1 Accuracy: 0.08597285067873303\n",
      "LP2 Accuracy: 0.3167420814479638\n",
      "Step 221/5000\n",
      "LP1 Accuracy: 0.08558558558558559\n",
      "LP2 Accuracy: 0.31981981981981983\n",
      "Step 222/5000\n",
      "LP1 Accuracy: 0.08520179372197309\n",
      "LP2 Accuracy: 0.3183856502242152\n",
      "Step 223/5000\n",
      "LP1 Accuracy: 0.08482142857142858\n",
      "LP2 Accuracy: 0.32142857142857145\n",
      "Step 224/5000\n",
      "LP1 Accuracy: 0.08444444444444445\n",
      "LP2 Accuracy: 0.32\n",
      "Step 225/5000\n",
      "LP1 Accuracy: 0.084070796460177\n",
      "LP2 Accuracy: 0.3185840707964602\n",
      "Step 226/5000\n",
      "LP1 Accuracy: 0.08370044052863436\n",
      "LP2 Accuracy: 0.32158590308370044\n",
      "Step 227/5000\n",
      "LP1 Accuracy: 0.08333333333333333\n",
      "LP2 Accuracy: 0.3201754385964912\n",
      "Step 228/5000\n",
      "LP1 Accuracy: 0.08296943231441048\n",
      "LP2 Accuracy: 0.31877729257641924\n",
      "Step 229/5000\n",
      "LP1 Accuracy: 0.08695652173913043\n",
      "LP2 Accuracy: 0.3173913043478261\n",
      "Step 230/5000\n",
      "LP1 Accuracy: 0.08658008658008658\n",
      "LP2 Accuracy: 0.31601731601731603\n",
      "Step 231/5000\n",
      "LP1 Accuracy: 0.09051724137931035\n",
      "LP2 Accuracy: 0.31896551724137934\n",
      "Step 232/5000\n",
      "LP1 Accuracy: 0.09012875536480687\n",
      "LP2 Accuracy: 0.3218884120171674\n",
      "Step 233/5000\n",
      "LP1 Accuracy: 0.08974358974358974\n",
      "LP2 Accuracy: 0.32051282051282054\n",
      "Step 234/5000\n",
      "LP1 Accuracy: 0.09361702127659574\n",
      "LP2 Accuracy: 0.32340425531914896\n",
      "Step 235/5000\n",
      "LP1 Accuracy: 0.09322033898305085\n",
      "LP2 Accuracy: 0.3220338983050847\n",
      "Step 236/5000\n",
      "LP1 Accuracy: 0.09282700421940929\n",
      "LP2 Accuracy: 0.3206751054852321\n",
      "Step 237/5000\n",
      "LP1 Accuracy: 0.09243697478991597\n",
      "LP2 Accuracy: 0.31932773109243695\n",
      "Step 238/5000\n",
      "LP1 Accuracy: 0.09205020920502092\n",
      "LP2 Accuracy: 0.3179916317991632\n",
      "Step 239/5000\n",
      "LP1 Accuracy: 0.09166666666666666\n",
      "LP2 Accuracy: 0.31666666666666665\n",
      "Step 240/5000\n",
      "LP1 Accuracy: 0.0912863070539419\n",
      "LP2 Accuracy: 0.3153526970954357\n",
      "Step 241/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.3181818181818182\n",
      "Step 242/5000\n",
      "LP1 Accuracy: 0.09053497942386832\n",
      "LP2 Accuracy: 0.3168724279835391\n",
      "Step 243/5000\n",
      "LP1 Accuracy: 0.09016393442622951\n",
      "LP2 Accuracy: 0.3155737704918033\n",
      "Step 244/5000\n",
      "LP1 Accuracy: 0.08979591836734693\n",
      "LP2 Accuracy: 0.3183673469387755\n",
      "Step 245/5000\n",
      "LP1 Accuracy: 0.08943089430894309\n",
      "LP2 Accuracy: 0.32113821138211385\n",
      "Step 246/5000\n",
      "LP1 Accuracy: 0.08906882591093117\n",
      "LP2 Accuracy: 0.32388663967611336\n",
      "Step 247/5000\n",
      "LP1 Accuracy: 0.08870967741935484\n",
      "LP2 Accuracy: 0.32661290322580644\n",
      "Step 248/5000\n",
      "LP1 Accuracy: 0.08835341365461848\n",
      "LP2 Accuracy: 0.3293172690763052\n",
      "Step 249/5000\n",
      "LP1 Accuracy: 0.088\n",
      "LP2 Accuracy: 0.328\n",
      "Step 250/5000\n",
      "LP1 Accuracy: 0.08764940239043825\n",
      "LP2 Accuracy: 0.32669322709163345\n",
      "Step 251/5000\n",
      "LP1 Accuracy: 0.0873015873015873\n",
      "LP2 Accuracy: 0.3253968253968254\n",
      "Step 252/5000\n",
      "LP1 Accuracy: 0.08695652173913043\n",
      "LP2 Accuracy: 0.3241106719367589\n",
      "Step 253/5000\n",
      "LP1 Accuracy: 0.09055118110236221\n",
      "LP2 Accuracy: 0.32677165354330706\n",
      "Step 254/5000\n",
      "LP1 Accuracy: 0.09019607843137255\n",
      "LP2 Accuracy: 0.32941176470588235\n",
      "Step 255/5000\n",
      "LP1 Accuracy: 0.08984375\n",
      "LP2 Accuracy: 0.328125\n",
      "Step 256/5000\n",
      "LP1 Accuracy: 0.08949416342412451\n",
      "LP2 Accuracy: 0.32684824902723736\n",
      "Step 257/5000\n",
      "LP1 Accuracy: 0.08914728682170543\n",
      "LP2 Accuracy: 0.32558139534883723\n",
      "Step 258/5000\n",
      "LP1 Accuracy: 0.0888030888030888\n",
      "LP2 Accuracy: 0.32432432432432434\n",
      "Step 259/5000\n",
      "LP1 Accuracy: 0.08846153846153847\n",
      "LP2 Accuracy: 0.3230769230769231\n",
      "Step 260/5000\n",
      "LP1 Accuracy: 0.08812260536398467\n",
      "LP2 Accuracy: 0.3218390804597701\n",
      "Step 261/5000\n",
      "LP1 Accuracy: 0.08778625954198473\n",
      "LP2 Accuracy: 0.3244274809160305\n",
      "Step 262/5000\n",
      "LP1 Accuracy: 0.08745247148288973\n",
      "LP2 Accuracy: 0.3269961977186312\n",
      "Step 263/5000\n",
      "LP1 Accuracy: 0.08712121212121213\n",
      "LP2 Accuracy: 0.32575757575757575\n",
      "Step 264/5000\n",
      "LP1 Accuracy: 0.08679245283018867\n",
      "LP2 Accuracy: 0.32452830188679244\n",
      "Step 265/5000\n",
      "LP1 Accuracy: 0.08646616541353383\n",
      "LP2 Accuracy: 0.3233082706766917\n",
      "Step 266/5000\n",
      "LP1 Accuracy: 0.08614232209737828\n",
      "LP2 Accuracy: 0.32209737827715357\n",
      "Step 267/5000\n",
      "LP1 Accuracy: 0.08582089552238806\n",
      "LP2 Accuracy: 0.3208955223880597\n",
      "Step 268/5000\n",
      "LP1 Accuracy: 0.08550185873605948\n",
      "LP2 Accuracy: 0.31970260223048325\n",
      "Step 269/5000\n",
      "LP1 Accuracy: 0.08518518518518518\n",
      "LP2 Accuracy: 0.32222222222222224\n",
      "Step 270/5000\n",
      "LP1 Accuracy: 0.08856088560885608\n",
      "LP2 Accuracy: 0.3210332103321033\n",
      "Step 271/5000\n",
      "LP1 Accuracy: 0.08823529411764706\n",
      "LP2 Accuracy: 0.3235294117647059\n",
      "Step 272/5000\n",
      "LP1 Accuracy: 0.09157509157509157\n",
      "LP2 Accuracy: 0.32234432234432236\n",
      "Step 273/5000\n",
      "LP1 Accuracy: 0.09124087591240876\n",
      "LP2 Accuracy: 0.32116788321167883\n",
      "Step 274/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.32\n",
      "Step 275/5000\n",
      "LP1 Accuracy: 0.09057971014492754\n",
      "LP2 Accuracy: 0.3188405797101449\n",
      "Step 276/5000\n",
      "LP1 Accuracy: 0.09386281588447654\n",
      "LP2 Accuracy: 0.3176895306859206\n",
      "Step 277/5000\n",
      "LP1 Accuracy: 0.09352517985611511\n",
      "LP2 Accuracy: 0.31654676258992803\n",
      "Step 278/5000\n",
      "LP1 Accuracy: 0.0931899641577061\n",
      "LP2 Accuracy: 0.3154121863799283\n",
      "Step 279/5000\n",
      "LP1 Accuracy: 0.09285714285714286\n",
      "LP2 Accuracy: 0.3142857142857143\n",
      "Step 280/5000\n",
      "LP1 Accuracy: 0.09252669039145907\n",
      "LP2 Accuracy: 0.31316725978647686\n",
      "Step 281/5000\n",
      "LP1 Accuracy: 0.09219858156028368\n",
      "LP2 Accuracy: 0.31560283687943264\n",
      "Step 282/5000\n",
      "LP1 Accuracy: 0.09187279151943463\n",
      "LP2 Accuracy: 0.31448763250883394\n",
      "Step 283/5000\n",
      "LP1 Accuracy: 0.09154929577464789\n",
      "LP2 Accuracy: 0.31338028169014087\n",
      "Step 284/5000\n",
      "LP1 Accuracy: 0.0912280701754386\n",
      "LP2 Accuracy: 0.3157894736842105\n",
      "Step 285/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.3146853146853147\n",
      "Step 286/5000\n",
      "LP1 Accuracy: 0.09059233449477352\n",
      "LP2 Accuracy: 0.313588850174216\n",
      "Step 287/5000\n",
      "LP1 Accuracy: 0.09027777777777778\n",
      "LP2 Accuracy: 0.3125\n",
      "Step 288/5000\n",
      "LP1 Accuracy: 0.08996539792387544\n",
      "LP2 Accuracy: 0.314878892733564\n",
      "Step 289/5000\n",
      "LP1 Accuracy: 0.0896551724137931\n",
      "LP2 Accuracy: 0.3137931034482759\n",
      "Step 290/5000\n",
      "LP1 Accuracy: 0.08934707903780069\n",
      "LP2 Accuracy: 0.3127147766323024\n",
      "Step 291/5000\n",
      "LP1 Accuracy: 0.08904109589041095\n",
      "LP2 Accuracy: 0.3116438356164384\n",
      "Step 292/5000\n",
      "LP1 Accuracy: 0.08873720136518772\n",
      "LP2 Accuracy: 0.31399317406143346\n",
      "Step 293/5000\n",
      "LP1 Accuracy: 0.08843537414965986\n",
      "LP2 Accuracy: 0.3129251700680272\n",
      "Step 294/5000\n",
      "LP1 Accuracy: 0.09152542372881356\n",
      "LP2 Accuracy: 0.3152542372881356\n",
      "Step 295/5000\n",
      "LP1 Accuracy: 0.09121621621621621\n",
      "LP2 Accuracy: 0.31756756756756754\n",
      "Step 296/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.3164983164983165\n",
      "Step 297/5000\n",
      "LP1 Accuracy: 0.09060402684563758\n",
      "LP2 Accuracy: 0.3187919463087248\n",
      "Step 298/5000\n",
      "LP1 Accuracy: 0.0903010033444816\n",
      "LP2 Accuracy: 0.3177257525083612\n",
      "Step 299/5000\n",
      "LP1 Accuracy: 0.09\n",
      "LP2 Accuracy: 0.31666666666666665\n",
      "Step 300/5000\n",
      "LP1 Accuracy: 0.08970099667774087\n",
      "LP2 Accuracy: 0.31561461794019935\n",
      "Step 301/5000\n",
      "LP1 Accuracy: 0.08940397350993377\n",
      "LP2 Accuracy: 0.31788079470198677\n",
      "Step 302/5000\n",
      "LP1 Accuracy: 0.0891089108910891\n",
      "LP2 Accuracy: 0.3201320132013201\n",
      "Step 303/5000\n",
      "LP1 Accuracy: 0.08881578947368421\n",
      "LP2 Accuracy: 0.3190789473684211\n",
      "Step 304/5000\n",
      "LP1 Accuracy: 0.08852459016393442\n",
      "LP2 Accuracy: 0.32131147540983607\n",
      "Step 305/5000\n",
      "LP1 Accuracy: 0.08823529411764706\n",
      "LP2 Accuracy: 0.3235294117647059\n",
      "Step 306/5000\n",
      "LP1 Accuracy: 0.09120521172638436\n",
      "LP2 Accuracy: 0.32247557003257327\n",
      "Step 307/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.32142857142857145\n",
      "Step 308/5000\n",
      "LP1 Accuracy: 0.09061488673139159\n",
      "LP2 Accuracy: 0.32038834951456313\n",
      "Step 309/5000\n",
      "LP1 Accuracy: 0.09032258064516129\n",
      "LP2 Accuracy: 0.3193548387096774\n",
      "Step 310/5000\n",
      "LP1 Accuracy: 0.09003215434083602\n",
      "LP2 Accuracy: 0.3215434083601286\n",
      "Step 311/5000\n",
      "LP1 Accuracy: 0.08974358974358974\n",
      "LP2 Accuracy: 0.32051282051282054\n",
      "Step 312/5000\n",
      "LP1 Accuracy: 0.08945686900958466\n",
      "LP2 Accuracy: 0.3194888178913738\n",
      "Step 313/5000\n",
      "LP1 Accuracy: 0.08917197452229299\n",
      "LP2 Accuracy: 0.3184713375796178\n",
      "Step 314/5000\n",
      "LP1 Accuracy: 0.08888888888888889\n",
      "LP2 Accuracy: 0.32063492063492066\n",
      "Step 315/5000\n",
      "LP1 Accuracy: 0.08860759493670886\n",
      "LP2 Accuracy: 0.31962025316455694\n",
      "Step 316/5000\n",
      "LP1 Accuracy: 0.0914826498422713\n",
      "LP2 Accuracy: 0.3186119873817035\n",
      "Step 317/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP1 Accuracy: 0.09119496855345911\n",
      "LP2 Accuracy: 0.31761006289308175\n",
      "Step 318/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.31974921630094044\n",
      "Step 319/5000\n",
      "LP1 Accuracy: 0.09375\n",
      "LP2 Accuracy: 0.321875\n",
      "Step 320/5000\n",
      "LP1 Accuracy: 0.09345794392523364\n",
      "LP2 Accuracy: 0.32087227414330216\n",
      "Step 321/5000\n",
      "LP1 Accuracy: 0.09316770186335403\n",
      "LP2 Accuracy: 0.3198757763975155\n",
      "Step 322/5000\n",
      "LP1 Accuracy: 0.09287925696594428\n",
      "LP2 Accuracy: 0.3188854489164087\n",
      "Step 323/5000\n",
      "LP1 Accuracy: 0.09259259259259259\n",
      "LP2 Accuracy: 0.32098765432098764\n",
      "Step 324/5000\n",
      "LP1 Accuracy: 0.09230769230769231\n",
      "LP2 Accuracy: 0.3230769230769231\n",
      "Step 325/5000\n",
      "LP1 Accuracy: 0.09202453987730061\n",
      "LP2 Accuracy: 0.3220858895705521\n",
      "Step 326/5000\n",
      "LP1 Accuracy: 0.09174311926605505\n",
      "LP2 Accuracy: 0.3211009174311927\n",
      "Step 327/5000\n",
      "LP1 Accuracy: 0.09146341463414634\n",
      "LP2 Accuracy: 0.3231707317073171\n",
      "Step 328/5000\n",
      "LP1 Accuracy: 0.0911854103343465\n",
      "LP2 Accuracy: 0.3252279635258359\n",
      "Step 329/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.3242424242424242\n",
      "Step 330/5000\n",
      "LP1 Accuracy: 0.09063444108761329\n",
      "LP2 Accuracy: 0.32326283987915405\n",
      "Step 331/5000\n",
      "LP1 Accuracy: 0.09036144578313253\n",
      "LP2 Accuracy: 0.3253012048192771\n",
      "Step 332/5000\n",
      "LP1 Accuracy: 0.09009009009009009\n",
      "LP2 Accuracy: 0.32732732732732733\n",
      "Step 333/5000\n",
      "LP1 Accuracy: 0.08982035928143713\n",
      "LP2 Accuracy: 0.3263473053892216\n",
      "Step 334/5000\n",
      "LP1 Accuracy: 0.08955223880597014\n",
      "LP2 Accuracy: 0.3253731343283582\n",
      "Step 335/5000\n",
      "LP1 Accuracy: 0.08928571428571429\n",
      "LP2 Accuracy: 0.3244047619047619\n",
      "Step 336/5000\n",
      "LP1 Accuracy: 0.08902077151335312\n",
      "LP2 Accuracy: 0.3264094955489614\n",
      "Step 337/5000\n",
      "LP1 Accuracy: 0.08875739644970414\n",
      "LP2 Accuracy: 0.32840236686390534\n",
      "Step 338/5000\n",
      "LP1 Accuracy: 0.08849557522123894\n",
      "LP2 Accuracy: 0.3274336283185841\n",
      "Step 339/5000\n",
      "LP1 Accuracy: 0.08823529411764706\n",
      "LP2 Accuracy: 0.3264705882352941\n",
      "Step 340/5000\n",
      "LP1 Accuracy: 0.08797653958944282\n",
      "LP2 Accuracy: 0.3255131964809384\n",
      "Step 341/5000\n",
      "LP1 Accuracy: 0.08771929824561403\n",
      "LP2 Accuracy: 0.32748538011695905\n",
      "Step 342/5000\n",
      "LP1 Accuracy: 0.09037900874635568\n",
      "LP2 Accuracy: 0.32653061224489793\n",
      "Step 343/5000\n",
      "LP1 Accuracy: 0.09011627906976744\n",
      "LP2 Accuracy: 0.32558139534883723\n",
      "Step 344/5000\n",
      "LP1 Accuracy: 0.08985507246376812\n",
      "LP2 Accuracy: 0.32753623188405795\n",
      "Step 345/5000\n",
      "LP1 Accuracy: 0.08959537572254335\n",
      "LP2 Accuracy: 0.3265895953757225\n",
      "Step 346/5000\n",
      "LP1 Accuracy: 0.0893371757925072\n",
      "LP2 Accuracy: 0.3256484149855908\n",
      "Step 347/5000\n",
      "LP1 Accuracy: 0.08908045977011494\n",
      "LP2 Accuracy: 0.32471264367816094\n",
      "Step 348/5000\n",
      "LP1 Accuracy: 0.08882521489971347\n",
      "LP2 Accuracy: 0.3237822349570201\n",
      "Step 349/5000\n",
      "LP1 Accuracy: 0.09142857142857143\n",
      "LP2 Accuracy: 0.32571428571428573\n",
      "Step 350/5000\n",
      "LP1 Accuracy: 0.09116809116809117\n",
      "LP2 Accuracy: 0.3247863247863248\n",
      "Step 351/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.32386363636363635\n",
      "Step 352/5000\n",
      "LP1 Accuracy: 0.0906515580736544\n",
      "LP2 Accuracy: 0.32294617563739375\n",
      "Step 353/5000\n",
      "LP1 Accuracy: 0.0903954802259887\n",
      "LP2 Accuracy: 0.3248587570621469\n",
      "Step 354/5000\n",
      "LP1 Accuracy: 0.09014084507042254\n",
      "LP2 Accuracy: 0.323943661971831\n",
      "Step 355/5000\n",
      "LP1 Accuracy: 0.0898876404494382\n",
      "LP2 Accuracy: 0.32303370786516855\n",
      "Step 356/5000\n",
      "LP1 Accuracy: 0.0896358543417367\n",
      "LP2 Accuracy: 0.32212885154061627\n",
      "Step 357/5000\n",
      "LP1 Accuracy: 0.09217877094972067\n",
      "LP2 Accuracy: 0.32122905027932963\n",
      "Step 358/5000\n",
      "LP1 Accuracy: 0.09192200557103064\n",
      "LP2 Accuracy: 0.3203342618384401\n",
      "Step 359/5000\n",
      "LP1 Accuracy: 0.09166666666666666\n",
      "LP2 Accuracy: 0.3194444444444444\n",
      "Step 360/5000\n",
      "LP1 Accuracy: 0.09141274238227147\n",
      "LP2 Accuracy: 0.32132963988919666\n",
      "Step 361/5000\n",
      "LP1 Accuracy: 0.09116022099447514\n",
      "LP2 Accuracy: 0.32044198895027626\n",
      "Step 362/5000\n",
      "LP1 Accuracy: 0.09090909090909091\n",
      "LP2 Accuracy: 0.31955922865013775\n",
      "Step 363/5000\n",
      "LP1 Accuracy: 0.09065934065934066\n",
      "LP2 Accuracy: 0.31868131868131866\n",
      "Step 364/5000\n",
      "LP1 Accuracy: 0.09041095890410959\n",
      "LP2 Accuracy: 0.32054794520547947\n",
      "Step 365/5000\n",
      "LP1 Accuracy: 0.09016393442622951\n",
      "LP2 Accuracy: 0.319672131147541\n",
      "Step 366/5000\n",
      "LP1 Accuracy: 0.08991825613079019\n",
      "LP2 Accuracy: 0.3215258855585831\n",
      "Step 367/5000\n",
      "LP1 Accuracy: 0.08967391304347826\n",
      "LP2 Accuracy: 0.32065217391304346\n",
      "Step 368/5000\n",
      "LP1 Accuracy: 0.08943089430894309\n",
      "LP2 Accuracy: 0.3224932249322493\n",
      "Step 369/5000\n",
      "LP1 Accuracy: 0.0891891891891892\n",
      "LP2 Accuracy: 0.3216216216216216\n",
      "Step 370/5000\n",
      "LP1 Accuracy: 0.0889487870619946\n",
      "LP2 Accuracy: 0.32075471698113206\n",
      "Step 371/5000\n",
      "LP1 Accuracy: 0.08870967741935484\n",
      "LP2 Accuracy: 0.31989247311827956\n",
      "Step 372/5000\n",
      "LP1 Accuracy: 0.08847184986595175\n",
      "LP2 Accuracy: 0.3190348525469169\n",
      "Step 373/5000\n",
      "LP1 Accuracy: 0.08823529411764706\n",
      "LP2 Accuracy: 0.32085561497326204\n",
      "Step 374/5000\n",
      "LP1 Accuracy: 0.088\n",
      "LP2 Accuracy: 0.32\n",
      "Step 375/5000\n",
      "LP1 Accuracy: 0.08776595744680851\n",
      "LP2 Accuracy: 0.32180851063829785\n",
      "Step 376/5000\n",
      "LP1 Accuracy: 0.08753315649867374\n",
      "LP2 Accuracy: 0.3209549071618037\n",
      "Step 377/5000\n",
      "LP1 Accuracy: 0.0873015873015873\n",
      "LP2 Accuracy: 0.3201058201058201\n",
      "Step 378/5000\n",
      "LP1 Accuracy: 0.0870712401055409\n",
      "LP2 Accuracy: 0.32189973614775724\n",
      "Step 379/5000\n",
      "LP1 Accuracy: 0.0868421052631579\n",
      "LP2 Accuracy: 0.32105263157894737\n",
      "Step 380/5000\n",
      "LP1 Accuracy: 0.08661417322834646\n",
      "LP2 Accuracy: 0.32020997375328086\n",
      "Step 381/5000\n",
      "LP1 Accuracy: 0.08638743455497382\n",
      "LP2 Accuracy: 0.3193717277486911\n",
      "Step 382/5000\n",
      "LP1 Accuracy: 0.08616187989556136\n",
      "LP2 Accuracy: 0.3185378590078329\n",
      "Step 383/5000\n",
      "LP1 Accuracy: 0.0859375\n",
      "LP2 Accuracy: 0.3177083333333333\n",
      "Step 384/5000\n",
      "LP1 Accuracy: 0.08571428571428572\n",
      "LP2 Accuracy: 0.3194805194805195\n",
      "Step 385/5000\n",
      "LP1 Accuracy: 0.08549222797927461\n",
      "LP2 Accuracy: 0.31865284974093266\n",
      "Step 386/5000\n",
      "LP1 Accuracy: 0.08527131782945736\n",
      "LP2 Accuracy: 0.3178294573643411\n",
      "Step 387/5000\n",
      "LP1 Accuracy: 0.08505154639175258\n",
      "LP2 Accuracy: 0.31958762886597936\n",
      "Step 388/5000\n",
      "LP1 Accuracy: 0.08483290488431877\n",
      "LP2 Accuracy: 0.31876606683804626\n",
      "Step 389/5000\n",
      "LP1 Accuracy: 0.08461538461538462\n",
      "LP2 Accuracy: 0.32051282051282054\n",
      "Step 390/5000\n",
      "LP1 Accuracy: 0.08695652173913043\n",
      "LP2 Accuracy: 0.32225063938618925\n",
      "Step 391/5000\n",
      "LP1 Accuracy: 0.08673469387755102\n",
      "LP2 Accuracy: 0.32142857142857145\n",
      "Step 392/5000\n",
      "LP1 Accuracy: 0.08651399491094147\n",
      "LP2 Accuracy: 0.32061068702290074\n",
      "Step 393/5000\n",
      "LP1 Accuracy: 0.08629441624365482\n",
      "LP2 Accuracy: 0.3223350253807107\n",
      "Step 394/5000\n",
      "LP1 Accuracy: 0.08607594936708861\n",
      "LP2 Accuracy: 0.32151898734177214\n",
      "Step 395/5000\n",
      "LP1 Accuracy: 0.08585858585858586\n",
      "LP2 Accuracy: 0.3207070707070707\n",
      "Step 396/5000\n",
      "LP1 Accuracy: 0.08564231738035265\n",
      "LP2 Accuracy: 0.3224181360201511\n",
      "Step 397/5000\n",
      "LP1 Accuracy: 0.08542713567839195\n",
      "LP2 Accuracy: 0.3241206030150754\n",
      "Step 398/5000\n",
      "LP1 Accuracy: 0.08521303258145363\n",
      "LP2 Accuracy: 0.3258145363408521\n",
      "Step 399/5000\n",
      "LP1 Accuracy: 0.085\n",
      "LP2 Accuracy: 0.3275\n",
      "Step 400/5000\n",
      "LP1 Accuracy: 0.08478802992518704\n",
      "LP2 Accuracy: 0.3266832917705736\n",
      "Step 401/5000\n",
      "LP1 Accuracy: 0.0845771144278607\n",
      "LP2 Accuracy: 0.32587064676616917\n",
      "Step 402/5000\n",
      "LP1 Accuracy: 0.08436724565756824\n",
      "LP2 Accuracy: 0.32754342431761785\n",
      "Step 403/5000\n",
      "LP1 Accuracy: 0.08415841584158416\n",
      "LP2 Accuracy: 0.32673267326732675\n",
      "Step 404/5000\n",
      "LP1 Accuracy: 0.08395061728395062\n",
      "LP2 Accuracy: 0.32839506172839505\n",
      "Step 405/5000\n",
      "LP1 Accuracy: 0.08374384236453201\n",
      "LP2 Accuracy: 0.3275862068965517\n",
      "Step 406/5000\n",
      "LP1 Accuracy: 0.08353808353808354\n",
      "LP2 Accuracy: 0.32923832923832924\n",
      "Step 407/5000\n",
      "LP1 Accuracy: 0.0857843137254902\n",
      "LP2 Accuracy: 0.3284313725490196\n",
      "Step 408/5000\n",
      "LP1 Accuracy: 0.08557457212713937\n",
      "LP2 Accuracy: 0.3276283618581907\n",
      "Step 409/5000\n",
      "LP1 Accuracy: 0.08536585365853659\n",
      "LP2 Accuracy: 0.32926829268292684\n",
      "Step 410/5000\n",
      "LP1 Accuracy: 0.0851581508515815\n",
      "LP2 Accuracy: 0.3284671532846715\n",
      "Step 411/5000\n",
      "LP1 Accuracy: 0.08495145631067962\n",
      "LP2 Accuracy: 0.3276699029126214\n",
      "Step 412/5000\n",
      "LP1 Accuracy: 0.0847457627118644\n",
      "LP2 Accuracy: 0.3268765133171913\n",
      "Step 413/5000\n",
      "LP1 Accuracy: 0.08454106280193237\n",
      "LP2 Accuracy: 0.3285024154589372\n",
      "Step 414/5000\n",
      "LP1 Accuracy: 0.08433734939759036\n",
      "LP2 Accuracy: 0.3301204819277108\n",
      "Step 415/5000\n",
      "LP1 Accuracy: 0.08413461538461539\n",
      "LP2 Accuracy: 0.3317307692307692\n",
      "Step 416/5000\n",
      "LP1 Accuracy: 0.08393285371702638\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 417/5000\n",
      "LP1 Accuracy: 0.08373205741626795\n",
      "LP2 Accuracy: 0.33253588516746413\n",
      "Step 418/5000\n",
      "LP1 Accuracy: 0.08353221957040573\n",
      "LP2 Accuracy: 0.3317422434367542\n",
      "Step 419/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP1 Accuracy: 0.08333333333333333\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 420/5000\n",
      "LP1 Accuracy: 0.0831353919239905\n",
      "LP2 Accuracy: 0.332541567695962\n",
      "Step 421/5000\n",
      "LP1 Accuracy: 0.08293838862559241\n",
      "LP2 Accuracy: 0.3341232227488152\n",
      "Step 422/5000\n",
      "LP1 Accuracy: 0.08274231678486997\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 423/5000\n",
      "LP1 Accuracy: 0.08254716981132075\n",
      "LP2 Accuracy: 0.33254716981132076\n",
      "Step 424/5000\n",
      "LP1 Accuracy: 0.08235294117647059\n",
      "LP2 Accuracy: 0.33176470588235296\n",
      "Step 425/5000\n",
      "LP1 Accuracy: 0.08215962441314555\n",
      "LP2 Accuracy: 0.33098591549295775\n",
      "Step 426/5000\n",
      "LP1 Accuracy: 0.08196721311475409\n",
      "LP2 Accuracy: 0.3325526932084309\n",
      "Step 427/5000\n",
      "LP1 Accuracy: 0.08177570093457943\n",
      "LP2 Accuracy: 0.3341121495327103\n",
      "Step 428/5000\n",
      "LP1 Accuracy: 0.08158508158508158\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 429/5000\n",
      "LP1 Accuracy: 0.08139534883720931\n",
      "LP2 Accuracy: 0.3325581395348837\n",
      "Step 430/5000\n",
      "LP1 Accuracy: 0.08120649651972157\n",
      "LP2 Accuracy: 0.33178654292343385\n",
      "Step 431/5000\n",
      "LP1 Accuracy: 0.08101851851851852\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 432/5000\n",
      "LP1 Accuracy: 0.08083140877598152\n",
      "LP2 Accuracy: 0.3325635103926097\n",
      "Step 433/5000\n",
      "LP1 Accuracy: 0.08064516129032258\n",
      "LP2 Accuracy: 0.3317972350230415\n",
      "Step 434/5000\n",
      "LP1 Accuracy: 0.08045977011494253\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 435/5000\n",
      "LP1 Accuracy: 0.08256880733944955\n",
      "LP2 Accuracy: 0.3348623853211009\n",
      "Step 436/5000\n",
      "LP1 Accuracy: 0.08237986270022883\n",
      "LP2 Accuracy: 0.33638443935926776\n",
      "Step 437/5000\n",
      "LP1 Accuracy: 0.08447488584474885\n",
      "LP2 Accuracy: 0.3356164383561644\n",
      "Step 438/5000\n",
      "LP1 Accuracy: 0.08428246013667426\n",
      "LP2 Accuracy: 0.3348519362186788\n",
      "Step 439/5000\n",
      "LP1 Accuracy: 0.08409090909090909\n",
      "LP2 Accuracy: 0.3340909090909091\n",
      "Step 440/5000\n",
      "LP1 Accuracy: 0.08390022675736962\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 441/5000\n",
      "LP1 Accuracy: 0.083710407239819\n",
      "LP2 Accuracy: 0.332579185520362\n",
      "Step 442/5000\n",
      "LP1 Accuracy: 0.0835214446952596\n",
      "LP2 Accuracy: 0.33182844243792325\n",
      "Step 443/5000\n",
      "LP1 Accuracy: 0.08333333333333333\n",
      "LP2 Accuracy: 0.3310810810810811\n",
      "Step 444/5000\n",
      "LP1 Accuracy: 0.08314606741573034\n",
      "LP2 Accuracy: 0.3303370786516854\n",
      "Step 445/5000\n",
      "LP1 Accuracy: 0.08295964125560538\n",
      "LP2 Accuracy: 0.3295964125560538\n",
      "Step 446/5000\n",
      "LP1 Accuracy: 0.08277404921700224\n",
      "LP2 Accuracy: 0.33109619686800895\n",
      "Step 447/5000\n",
      "LP1 Accuracy: 0.08258928571428571\n",
      "LP2 Accuracy: 0.3325892857142857\n",
      "Step 448/5000\n",
      "LP1 Accuracy: 0.08240534521158129\n",
      "LP2 Accuracy: 0.33184855233853006\n",
      "Step 449/5000\n",
      "LP1 Accuracy: 0.08222222222222222\n",
      "LP2 Accuracy: 0.33111111111111113\n",
      "Step 450/5000\n",
      "LP1 Accuracy: 0.082039911308204\n",
      "LP2 Accuracy: 0.3325942350332594\n",
      "Step 451/5000\n",
      "LP1 Accuracy: 0.084070796460177\n",
      "LP2 Accuracy: 0.334070796460177\n",
      "Step 452/5000\n",
      "LP1 Accuracy: 0.08388520971302428\n",
      "LP2 Accuracy: 0.3355408388520971\n",
      "Step 453/5000\n",
      "LP1 Accuracy: 0.08370044052863436\n",
      "LP2 Accuracy: 0.3370044052863436\n",
      "Step 454/5000\n",
      "LP1 Accuracy: 0.08351648351648351\n",
      "LP2 Accuracy: 0.3362637362637363\n",
      "Step 455/5000\n",
      "LP1 Accuracy: 0.08333333333333333\n",
      "LP2 Accuracy: 0.33771929824561403\n",
      "Step 456/5000\n",
      "LP1 Accuracy: 0.08533916849015317\n",
      "LP2 Accuracy: 0.33916849015317285\n",
      "Step 457/5000\n",
      "LP1 Accuracy: 0.0851528384279476\n",
      "LP2 Accuracy: 0.3384279475982533\n",
      "Step 458/5000\n",
      "LP1 Accuracy: 0.08496732026143791\n",
      "LP2 Accuracy: 0.33986928104575165\n",
      "Step 459/5000\n",
      "LP1 Accuracy: 0.08478260869565217\n",
      "LP2 Accuracy: 0.3391304347826087\n",
      "Step 460/5000\n",
      "LP1 Accuracy: 0.08459869848156182\n",
      "LP2 Accuracy: 0.3383947939262473\n",
      "Step 461/5000\n",
      "LP1 Accuracy: 0.08441558441558442\n",
      "LP2 Accuracy: 0.33766233766233766\n",
      "Step 462/5000\n",
      "LP1 Accuracy: 0.08423326133909287\n",
      "LP2 Accuracy: 0.3369330453563715\n",
      "Step 463/5000\n",
      "LP1 Accuracy: 0.08405172413793104\n",
      "LP2 Accuracy: 0.33836206896551724\n",
      "Step 464/5000\n",
      "LP1 Accuracy: 0.08387096774193549\n",
      "LP2 Accuracy: 0.33763440860215055\n",
      "Step 465/5000\n",
      "LP1 Accuracy: 0.08369098712446352\n",
      "LP2 Accuracy: 0.3369098712446352\n",
      "Step 466/5000\n",
      "LP1 Accuracy: 0.08565310492505353\n",
      "LP2 Accuracy: 0.3361884368308351\n",
      "Step 467/5000\n",
      "LP1 Accuracy: 0.0876068376068376\n",
      "LP2 Accuracy: 0.33547008547008544\n",
      "Step 468/5000\n",
      "LP1 Accuracy: 0.08742004264392324\n",
      "LP2 Accuracy: 0.3347547974413646\n",
      "Step 469/5000\n",
      "LP1 Accuracy: 0.08723404255319149\n",
      "LP2 Accuracy: 0.33404255319148934\n",
      "Step 470/5000\n",
      "LP1 Accuracy: 0.0870488322717622\n",
      "LP2 Accuracy: 0.3354564755838641\n",
      "Step 471/5000\n",
      "LP1 Accuracy: 0.08686440677966102\n",
      "LP2 Accuracy: 0.336864406779661\n",
      "Step 472/5000\n",
      "LP1 Accuracy: 0.08668076109936575\n",
      "LP2 Accuracy: 0.3361522198731501\n",
      "Step 473/5000\n",
      "LP1 Accuracy: 0.08649789029535865\n",
      "LP2 Accuracy: 0.33755274261603374\n",
      "Step 474/5000\n",
      "LP1 Accuracy: 0.0863157894736842\n",
      "LP2 Accuracy: 0.3389473684210526\n",
      "Step 475/5000\n",
      "LP1 Accuracy: 0.0861344537815126\n",
      "LP2 Accuracy: 0.3403361344537815\n",
      "Step 476/5000\n",
      "LP1 Accuracy: 0.0859538784067086\n",
      "LP2 Accuracy: 0.33962264150943394\n",
      "Step 477/5000\n",
      "LP1 Accuracy: 0.08577405857740586\n",
      "LP2 Accuracy: 0.3389121338912134\n",
      "Step 478/5000\n",
      "LP1 Accuracy: 0.08559498956158663\n",
      "LP2 Accuracy: 0.34029227557411273\n",
      "Step 479/5000\n",
      "LP1 Accuracy: 0.08541666666666667\n",
      "LP2 Accuracy: 0.3416666666666667\n",
      "Step 480/5000\n",
      "LP1 Accuracy: 0.08523908523908524\n",
      "LP2 Accuracy: 0.340956340956341\n",
      "Step 481/5000\n",
      "LP1 Accuracy: 0.08506224066390042\n",
      "LP2 Accuracy: 0.34024896265560167\n",
      "Step 482/5000\n",
      "LP1 Accuracy: 0.08488612836438923\n",
      "LP2 Accuracy: 0.33954451345755693\n",
      "Step 483/5000\n",
      "LP1 Accuracy: 0.08471074380165289\n",
      "LP2 Accuracy: 0.3409090909090909\n",
      "Step 484/5000\n",
      "LP1 Accuracy: 0.08453608247422681\n",
      "LP2 Accuracy: 0.3422680412371134\n",
      "Step 485/5000\n",
      "LP1 Accuracy: 0.08436213991769548\n",
      "LP2 Accuracy: 0.34156378600823045\n",
      "Step 486/5000\n",
      "LP1 Accuracy: 0.08418891170431211\n",
      "LP2 Accuracy: 0.3408624229979466\n",
      "Step 487/5000\n",
      "LP1 Accuracy: 0.0860655737704918\n",
      "LP2 Accuracy: 0.3401639344262295\n",
      "Step 488/5000\n",
      "LP1 Accuracy: 0.08588957055214724\n",
      "LP2 Accuracy: 0.3394683026584867\n",
      "Step 489/5000\n",
      "LP1 Accuracy: 0.08571428571428572\n",
      "LP2 Accuracy: 0.33877551020408164\n",
      "Step 490/5000\n",
      "LP1 Accuracy: 0.08757637474541752\n",
      "LP2 Accuracy: 0.3380855397148676\n",
      "Step 491/5000\n",
      "LP1 Accuracy: 0.08739837398373984\n",
      "LP2 Accuracy: 0.33739837398373984\n",
      "Step 492/5000\n",
      "LP1 Accuracy: 0.0872210953346856\n",
      "LP2 Accuracy: 0.3367139959432049\n",
      "Step 493/5000\n",
      "LP1 Accuracy: 0.08704453441295547\n",
      "LP2 Accuracy: 0.3360323886639676\n",
      "Step 494/5000\n",
      "LP1 Accuracy: 0.08888888888888889\n",
      "LP2 Accuracy: 0.3373737373737374\n",
      "Step 495/5000\n",
      "LP1 Accuracy: 0.08870967741935484\n",
      "LP2 Accuracy: 0.3387096774193548\n",
      "Step 496/5000\n",
      "LP1 Accuracy: 0.08853118712273642\n",
      "LP2 Accuracy: 0.3380281690140845\n",
      "Step 497/5000\n",
      "LP1 Accuracy: 0.08835341365461848\n",
      "LP2 Accuracy: 0.3373493975903614\n",
      "Step 498/5000\n",
      "LP1 Accuracy: 0.08817635270541083\n",
      "LP2 Accuracy: 0.33867735470941884\n",
      "Step 499/5000\n",
      "LP1 Accuracy: 0.088\n",
      "LP2 Accuracy: 0.338\n",
      "Step 500/5000\n",
      "LP1 Accuracy: 0.08782435129740519\n",
      "LP2 Accuracy: 0.3373253493013972\n",
      "Step 501/5000\n",
      "LP1 Accuracy: 0.08764940239043825\n",
      "LP2 Accuracy: 0.33665338645418325\n",
      "Step 502/5000\n",
      "LP1 Accuracy: 0.0874751491053678\n",
      "LP2 Accuracy: 0.3359840954274354\n",
      "Step 503/5000\n",
      "LP1 Accuracy: 0.0873015873015873\n",
      "LP2 Accuracy: 0.3353174603174603\n",
      "Step 504/5000\n",
      "LP1 Accuracy: 0.0891089108910891\n",
      "LP2 Accuracy: 0.3346534653465347\n",
      "Step 505/5000\n",
      "LP1 Accuracy: 0.08893280632411067\n",
      "LP2 Accuracy: 0.3339920948616601\n",
      "Step 506/5000\n",
      "LP1 Accuracy: 0.08875739644970414\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 507/5000\n",
      "LP1 Accuracy: 0.08858267716535433\n",
      "LP2 Accuracy: 0.33267716535433073\n",
      "Step 508/5000\n",
      "LP1 Accuracy: 0.08840864440078586\n",
      "LP2 Accuracy: 0.3320235756385069\n",
      "Step 509/5000\n",
      "LP1 Accuracy: 0.08823529411764706\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 510/5000\n",
      "LP1 Accuracy: 0.08806262230919765\n",
      "LP2 Accuracy: 0.33463796477495106\n",
      "Step 511/5000\n",
      "LP1 Accuracy: 0.08984375\n",
      "LP2 Accuracy: 0.333984375\n",
      "Step 512/5000\n",
      "LP1 Accuracy: 0.08966861598440545\n",
      "LP2 Accuracy: 0.3333333333333333\n",
      "Step 513/5000\n",
      "LP1 Accuracy: 0.08949416342412451\n",
      "LP2 Accuracy: 0.3326848249027237\n",
      "Step 514/5000\n",
      "LP1 Accuracy: 0.08932038834951456\n",
      "LP2 Accuracy: 0.3320388349514563\n",
      "Step 515/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-ccbdab8220ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#print(f\"Correct Label: {label}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdist_lp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLPDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcifar10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdist_lp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLPDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcifar10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlabel_lp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClosest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_lp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(f\"LP1 label = {label_lp1}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-244-22a0cd3112d1>\u001b[0m in \u001b[0;36mLPDist\u001b[0;34m(image, dataset, k, lp)\u001b[0m\n\u001b[1;32m     12\u001b[0m                     ),# sum\n\u001b[1;32m     13\u001b[0m             1/lp) # pow \n\u001b[0;32m---> 14\u001b[0;31m          for i in range(len(dataset))])\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#return dist, torch.argmin(dist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-244-22a0cd3112d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m                     ),# sum\n\u001b[1;32m     13\u001b[0m             1/lp) # pow \n\u001b[0;32m---> 14\u001b[0;31m          for i in range(len(dataset))])\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#return dist, torch.argmin(dist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tobytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2731\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct_lp1 = 0\n",
    "correct_lp2 = 0\n",
    "total = 0\n",
    "for idx, (img, label) in enumerate(dataSet):\n",
    "    print(f\"Step {idx}/{len(dataSet)}\")\n",
    "    #print(f\"Correct Label: {label}\")\n",
    "    dist_lp1, min_lp1 = LPDist(img,cifar10,k=k,lp=1)\n",
    "    dist_lp2, min_lp2 = LPDist(img,cifar10,k=k,lp=2)\n",
    "    label_lp1 = Closest(min_lp1, cifar10)\n",
    "    #print(f\"LP1 label = {label_lp1}\")\n",
    "    label_lp2 = Closest(min_lp2, cifar10)\n",
    "    #print(f\"LP2 label = {label_lp2}\")\n",
    "    correct_lp1 += getAccuracy(label, label_lp1)\n",
    "    correct_lp2 += getAccuracy(label, label_lp2)\n",
    "    total += bs\n",
    "    print(f\"LP1 Accuracy: {correct_lp1/total}\")\n",
    "    print(f\"LP2 Accuracy: {correct_lp2/total}\")\n",
    "\n",
    "print(\"Complete\")\n",
    "print(f\"LP1 Accuracy: {correct_lp1/total}\")\n",
    "print(f\"LP2 Accuracy: {correct_lp2/total}\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "LP1 Accuracy: 0.089\n",
    "\n",
    "LP2 Accuracy: 0.3320\n",
    "\n",
    "Note: This takes a long time to run. Stopped where I did because my computer can't crunch very fast and it appeared the results were converging anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: (8/15 pts)\n",
    "\n",
    "Implement a Convolutional Neural Network for CIFAR-10 dataset.\n",
    "\n",
    "You can follow this\n",
    "[tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n",
    "if need: \n",
    "\n",
    "Implement different network architectures and report your results. (for optimization, you can just following the tutorial, but feel free to experiment with different ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using 50000 images\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "epochs = 20\n",
    "bs = 10 # Batch Size \n",
    "numworkers = 5 # number of workers\n",
    "testPercent = 1 # % of dataset to use\n",
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "])\n",
    "cifar10 = torchvision.datasets.CIFAR10(\"../datasets\",train=True,download=True, transform=transform)\n",
    "\n",
    "indices = np.arange(len(cifar10))\n",
    "np.random.shuffle(indices)\n",
    "size = int(testPercent*len(cifar10))\n",
    "ind = indices[:size]\n",
    "print(f\"Using {size} images\")\n",
    "\n",
    "sampler = SRS(ind)\n",
    "\n",
    "dataSet = DataLoader(cifar10,\n",
    "                     batch_size=bs,\n",
    "                     sampler=sampler,\n",
    "                     shuffle=False,\n",
    "                     num_workers=numworkers,\n",
    "                     )\n",
    "cifar10Test = torchvision.datasets.CIFAR10(\"../datasets\",train=False,download=True, transform=transform)\n",
    "testSet = DataLoader(cifar10Test,\n",
    "                      batch_size=bs,\n",
    "                      shuffle=True,\n",
    "                      num_workers=numworkers,\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convBlock(_in,_out,kernel_size=3,\n",
    "              stride=1,\n",
    "              padding=1,\n",
    "              inplace=True,\n",
    "              dropout=False,\n",
    "              p=0.5,\n",
    "             ):\n",
    "    \n",
    "    layers = [nn.Conv2d(_in,_out,kernel_size=kernel_size,stride=stride,padding=padding)]\n",
    "    layers.append(nn.ReLU(inplace=inplace))\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout(p=p,inplace=inplace))\n",
    "        \n",
    "    return layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.reg = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.reg(self.conv1(x)))\n",
    "        x = self.pool(self.reg(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.reg(self.fc1(x))\n",
    "        x = self.reg(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.150\n",
      "[1,  2000] loss: 2.253\n",
      "[1,  3000] loss: 3.257\n",
      "[1,  4000] loss: 4.187\n",
      "[1,  5000] loss: 5.053\n",
      "Epoch Loss: 2.0210932716369627\n",
      "[2,  1000] loss: 0.827\n",
      "[2,  2000] loss: 1.628\n",
      "[2,  3000] loss: 2.402\n",
      "[2,  4000] loss: 3.152\n",
      "[2,  5000] loss: 3.900\n",
      "Epoch Loss: 1.5600294486641884\n",
      "[3,  1000] loss: 0.723\n",
      "[3,  2000] loss: 1.444\n",
      "[3,  3000] loss: 2.152\n",
      "[3,  4000] loss: 2.847\n",
      "[3,  5000] loss: 3.529\n",
      "Epoch Loss: 1.4116766491293906\n",
      "[4,  1000] loss: 0.672\n",
      "[4,  2000] loss: 1.345\n",
      "[4,  3000] loss: 2.001\n",
      "[4,  4000] loss: 2.659\n",
      "[4,  5000] loss: 3.316\n",
      "Epoch Loss: 1.3262041688799857\n",
      "[5,  1000] loss: 0.637\n",
      "[5,  2000] loss: 1.264\n",
      "[5,  3000] loss: 1.889\n",
      "[5,  4000] loss: 2.513\n",
      "[5,  5000] loss: 3.137\n",
      "Epoch Loss: 1.254721093505621\n",
      "[6,  1000] loss: 0.600\n",
      "[6,  2000] loss: 1.202\n",
      "[6,  3000] loss: 1.787\n",
      "[6,  4000] loss: 2.390\n",
      "[6,  5000] loss: 2.982\n",
      "Epoch Loss: 1.192643980705738\n",
      "[7,  1000] loss: 0.565\n",
      "[7,  2000] loss: 1.133\n",
      "[7,  3000] loss: 1.698\n",
      "[7,  4000] loss: 2.275\n",
      "[7,  5000] loss: 2.849\n",
      "Epoch Loss: 1.1395798569321633\n",
      "[8,  1000] loss: 0.546\n",
      "[8,  2000] loss: 1.091\n",
      "[8,  3000] loss: 1.633\n",
      "[8,  4000] loss: 2.172\n",
      "[8,  5000] loss: 2.719\n",
      "Epoch Loss: 1.0874829694777728\n",
      "[9,  1000] loss: 0.510\n",
      "[9,  2000] loss: 1.033\n",
      "[9,  3000] loss: 1.554\n",
      "[9,  4000] loss: 2.085\n",
      "[9,  5000] loss: 2.612\n",
      "Epoch Loss: 1.0446246004372834\n",
      "[10,  1000] loss: 0.495\n",
      "[10,  2000] loss: 0.990\n",
      "[10,  3000] loss: 1.496\n",
      "[10,  4000] loss: 2.007\n",
      "[10,  5000] loss: 2.506\n",
      "Epoch Loss: 1.0025621017992496\n",
      "[11,  1000] loss: 0.472\n",
      "[11,  2000] loss: 0.960\n",
      "[11,  3000] loss: 1.440\n",
      "[11,  4000] loss: 1.930\n",
      "[11,  5000] loss: 2.418\n",
      "Epoch Loss: 0.9670660886347294\n",
      "[12,  1000] loss: 0.454\n",
      "[12,  2000] loss: 0.911\n",
      "[12,  3000] loss: 1.375\n",
      "[12,  4000] loss: 1.842\n",
      "[12,  5000] loss: 2.319\n",
      "Epoch Loss: 0.9277072149127722\n",
      "[13,  1000] loss: 0.432\n",
      "[13,  2000] loss: 0.873\n",
      "[13,  3000] loss: 1.323\n",
      "[13,  4000] loss: 1.788\n",
      "[13,  5000] loss: 2.242\n",
      "Epoch Loss: 0.896610552200675\n",
      "[14,  1000] loss: 0.419\n",
      "[14,  2000] loss: 0.844\n",
      "[14,  3000] loss: 1.279\n",
      "[14,  4000] loss: 1.720\n",
      "[14,  5000] loss: 2.164\n",
      "Epoch Loss: 0.8655988729938865\n",
      "[15,  1000] loss: 0.390\n",
      "[15,  2000] loss: 0.810\n",
      "[15,  3000] loss: 1.240\n",
      "[15,  4000] loss: 1.664\n",
      "[15,  5000] loss: 2.091\n",
      "Epoch Loss: 0.8364907623939216\n",
      "[16,  1000] loss: 0.379\n",
      "[16,  2000] loss: 0.789\n",
      "[16,  3000] loss: 1.193\n",
      "[16,  4000] loss: 1.614\n",
      "[16,  5000] loss: 2.027\n",
      "Epoch Loss: 0.8108036170035601\n",
      "[17,  1000] loss: 0.378\n",
      "[17,  2000] loss: 0.754\n",
      "[17,  3000] loss: 1.147\n",
      "[17,  4000] loss: 1.550\n",
      "[17,  5000] loss: 1.958\n",
      "Epoch Loss: 0.7832250101879239\n",
      "[18,  1000] loss: 0.363\n",
      "[18,  2000] loss: 0.731\n",
      "[18,  3000] loss: 1.113\n",
      "[18,  4000] loss: 1.497\n",
      "[18,  5000] loss: 1.894\n",
      "Epoch Loss: 0.7577422723948956\n",
      "[19,  1000] loss: 0.348\n",
      "[19,  2000] loss: 0.698\n",
      "[19,  3000] loss: 1.080\n",
      "[19,  4000] loss: 1.462\n",
      "[19,  5000] loss: 1.842\n",
      "Epoch Loss: 0.7366505274415016\n",
      "[20,  1000] loss: 0.334\n",
      "[20,  2000] loss: 0.682\n",
      "[20,  3000] loss: 1.043\n",
      "[20,  4000] loss: 1.415\n",
      "[20,  5000] loss: 1.793\n",
      "Epoch Loss: 0.7170367607146502\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "net = ExampleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataSet, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            #running_loss = 0.0\n",
    "    print(f\"Epoch Loss: {running_loss/(i+1)}\")\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1992270113900303\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "running_loss = 0\n",
    "for i, (img, label) in enumerate(testSet):\n",
    "    pred = net(img)\n",
    "    loss = criterion(pred,label)\n",
    "    running_loss += loss.item()\n",
    "print(f\"Test Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss: 1.1992270113900303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shallowNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(shallowNetwork,self).__init__()\n",
    "        convLayers = []\n",
    "        for i in range(3):\n",
    "            convLayers += convBlock(3,3)\n",
    "        self.convs= nn.Sequential(*convLayers)\n",
    "        self.size = 3*32*32 # Size of single image\n",
    "        self.linear = nn.Linear(self.size, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1,self.size)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.155\n",
      "[1,  2000] loss: 2.307\n",
      "[1,  3000] loss: 3.460\n",
      "[1,  4000] loss: 4.612\n",
      "[1,  5000] loss: 5.765\n",
      "Epoch 0 Loss: 2.3060174372673035\n",
      "[2,  1000] loss: 1.152\n",
      "[2,  2000] loss: 2.304\n",
      "[2,  3000] loss: 3.457\n",
      "[2,  4000] loss: 4.610\n",
      "[2,  5000] loss: 5.762\n",
      "Epoch 1 Loss: 2.3048549178600313\n",
      "[3,  1000] loss: 1.153\n",
      "[3,  2000] loss: 2.305\n",
      "[3,  3000] loss: 3.458\n",
      "[3,  4000] loss: 4.610\n",
      "[3,  5000] loss: 5.762\n",
      "Epoch 2 Loss: 2.3047721650123596\n",
      "[4,  1000] loss: 1.153\n",
      "[4,  2000] loss: 2.305\n",
      "[4,  3000] loss: 3.458\n",
      "[4,  4000] loss: 4.611\n",
      "[4,  5000] loss: 5.763\n",
      "Epoch 3 Loss: 2.305278517484665\n",
      "[5,  1000] loss: 1.152\n",
      "[5,  2000] loss: 2.305\n",
      "[5,  3000] loss: 3.458\n",
      "[5,  4000] loss: 4.610\n",
      "[5,  5000] loss: 5.763\n",
      "Epoch 4 Loss: 2.305134665584564\n",
      "[6,  1000] loss: 1.152\n",
      "[6,  2000] loss: 2.305\n",
      "[6,  3000] loss: 3.457\n",
      "[6,  4000] loss: 4.609\n",
      "[6,  5000] loss: 5.761\n",
      "Epoch 5 Loss: 2.304594834518433\n",
      "[7,  1000] loss: 1.153\n",
      "[7,  2000] loss: 2.305\n",
      "[7,  3000] loss: 3.458\n",
      "[7,  4000] loss: 4.610\n",
      "[7,  5000] loss: 5.763\n",
      "Epoch 6 Loss: 2.3051411647319795\n",
      "[8,  1000] loss: 1.152\n",
      "[8,  2000] loss: 2.305\n",
      "[8,  3000] loss: 3.457\n",
      "[8,  4000] loss: 4.610\n",
      "[8,  5000] loss: 5.763\n",
      "Epoch 7 Loss: 2.30507944521904\n",
      "[9,  1000] loss: 1.153\n",
      "[9,  2000] loss: 2.305\n",
      "[9,  3000] loss: 3.458\n",
      "[9,  4000] loss: 4.610\n",
      "[9,  5000] loss: 5.763\n",
      "Epoch 8 Loss: 2.305101027727127\n",
      "[10,  1000] loss: 1.152\n",
      "[10,  2000] loss: 2.305\n",
      "[10,  3000] loss: 3.457\n",
      "[10,  4000] loss: 4.610\n",
      "[10,  5000] loss: 5.762\n",
      "Epoch 9 Loss: 2.3048953083992005\n",
      "[11,  1000] loss: 1.152\n",
      "[11,  2000] loss: 2.305\n",
      "[11,  3000] loss: 3.458\n",
      "[11,  4000] loss: 4.610\n",
      "[11,  5000] loss: 5.763\n",
      "Epoch 10 Loss: 2.3050133123874663\n",
      "[12,  1000] loss: 1.153\n",
      "[12,  2000] loss: 2.305\n",
      "[12,  3000] loss: 3.458\n",
      "[12,  4000] loss: 4.611\n",
      "[12,  5000] loss: 5.763\n",
      "Epoch 11 Loss: 2.305212775182724\n",
      "[13,  1000] loss: 1.152\n",
      "[13,  2000] loss: 2.305\n",
      "[13,  3000] loss: 3.457\n",
      "[13,  4000] loss: 4.610\n",
      "[13,  5000] loss: 5.762\n",
      "Epoch 12 Loss: 2.3048842123031617\n",
      "[14,  1000] loss: 1.152\n",
      "[14,  2000] loss: 2.305\n",
      "[14,  3000] loss: 3.457\n",
      "[14,  4000] loss: 4.610\n",
      "[14,  5000] loss: 5.763\n",
      "Epoch 13 Loss: 2.305044595050812\n",
      "[15,  1000] loss: 1.153\n",
      "[15,  2000] loss: 2.305\n",
      "[15,  3000] loss: 3.458\n",
      "[15,  4000] loss: 4.610\n",
      "[15,  5000] loss: 5.763\n",
      "Epoch 14 Loss: 2.3051096861839295\n",
      "[16,  1000] loss: 1.153\n",
      "[16,  2000] loss: 2.305\n",
      "[16,  3000] loss: 3.458\n",
      "[16,  4000] loss: 4.610\n",
      "[16,  5000] loss: 5.763\n",
      "Epoch 15 Loss: 2.3051877962589264\n",
      "[17,  1000] loss: 1.153\n",
      "[17,  2000] loss: 2.305\n",
      "[17,  3000] loss: 3.457\n",
      "[17,  4000] loss: 4.610\n",
      "[17,  5000] loss: 5.762\n",
      "Epoch 16 Loss: 2.304923085165024\n",
      "[18,  1000] loss: 1.152\n",
      "[18,  2000] loss: 2.305\n",
      "[18,  3000] loss: 3.457\n",
      "[18,  4000] loss: 4.610\n",
      "[18,  5000] loss: 5.763\n",
      "Epoch 17 Loss: 2.305105962228775\n",
      "[19,  1000] loss: 1.153\n",
      "[19,  2000] loss: 2.305\n",
      "[19,  3000] loss: 3.458\n",
      "[19,  4000] loss: 4.611\n",
      "[19,  5000] loss: 5.763\n",
      "Epoch 18 Loss: 2.305298063135147\n",
      "[20,  1000] loss: 1.153\n",
      "[20,  2000] loss: 2.305\n",
      "[20,  3000] loss: 3.458\n",
      "[20,  4000] loss: 4.610\n",
      "[20,  5000] loss: 5.763\n",
      "Epoch 19 Loss: 2.3053177502632143\n"
     ]
    }
   ],
   "source": [
    "net = shallowNetwork()\n",
    "net.train()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.01)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data,label) in enumerate(dataSet):\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(data)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "    print(f\"Epoch {epoch} Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.306026217699051\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "running_loss = 0\n",
    "for i, (img, label) in enumerate(testSet):\n",
    "    pred = net(img)\n",
    "    loss = criterion(pred,label)\n",
    "    running_loss += loss.item()\n",
    "print(f\"Test Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss: 2.306026217699051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperNetwork(nn.Module):\n",
    "    def __init__(self,numLayers=10):\n",
    "        super(DeeperNetwork,self).__init__()\n",
    "        convLayers = []\n",
    "        for i in range(numLayers):\n",
    "            convLayers += convBlock(3,3)\n",
    "        self.convs= nn.Sequential(*convLayers)\n",
    "        self.size = 3*32*32 # Size of single image\n",
    "        self.linear = nn.Sequential(nn.Linear(self.size,10),nn.ReLU())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1,self.size)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.152\n",
      "[1,  2000] loss: 2.304\n",
      "[1,  3000] loss: 3.455\n",
      "[1,  4000] loss: 4.606\n",
      "[1,  5000] loss: 5.758\n",
      "Epoch 0 Loss: 2.303054624128342\n",
      "[2,  1000] loss: 1.151\n",
      "[2,  2000] loss: 2.303\n",
      "[2,  3000] loss: 3.454\n",
      "[2,  4000] loss: 4.605\n",
      "[2,  5000] loss: 5.756\n",
      "Epoch 1 Loss: 2.3025853633880615\n",
      "[3,  1000] loss: 1.151\n",
      "[3,  2000] loss: 2.303\n",
      "[3,  3000] loss: 3.454\n",
      "[3,  4000] loss: 4.605\n",
      "[3,  5000] loss: 5.756\n",
      "Epoch 2 Loss: 2.3025853633880615\n",
      "[4,  1000] loss: 1.151\n",
      "[4,  2000] loss: 2.303\n",
      "[4,  3000] loss: 3.454\n",
      "[4,  4000] loss: 4.605\n",
      "[4,  5000] loss: 5.756\n",
      "Epoch 3 Loss: 2.3025853633880615\n",
      "[5,  1000] loss: 1.151\n",
      "[5,  2000] loss: 2.303\n",
      "[5,  3000] loss: 3.454\n",
      "[5,  4000] loss: 4.605\n",
      "[5,  5000] loss: 5.756\n",
      "Epoch 4 Loss: 2.3025853633880615\n",
      "[6,  1000] loss: 1.151\n",
      "[6,  2000] loss: 2.303\n",
      "[6,  3000] loss: 3.454\n",
      "[6,  4000] loss: 4.605\n",
      "[6,  5000] loss: 5.756\n",
      "Epoch 5 Loss: 2.3025853633880615\n",
      "[7,  1000] loss: 1.151\n",
      "[7,  2000] loss: 2.303\n",
      "[7,  3000] loss: 3.454\n",
      "[7,  4000] loss: 4.605\n",
      "[7,  5000] loss: 5.756\n",
      "Epoch 6 Loss: 2.3025853633880615\n",
      "[8,  1000] loss: 1.151\n",
      "[8,  2000] loss: 2.303\n",
      "[8,  3000] loss: 3.454\n",
      "[8,  4000] loss: 4.605\n",
      "[8,  5000] loss: 5.756\n",
      "Epoch 7 Loss: 2.3025853633880615\n",
      "[9,  1000] loss: 1.151\n",
      "[9,  2000] loss: 2.303\n",
      "[9,  3000] loss: 3.454\n",
      "[9,  4000] loss: 4.605\n",
      "[9,  5000] loss: 5.756\n",
      "Epoch 8 Loss: 2.3025853633880615\n",
      "[10,  1000] loss: 1.151\n",
      "[10,  2000] loss: 2.303\n",
      "[10,  3000] loss: 3.454\n",
      "[10,  4000] loss: 4.605\n",
      "[10,  5000] loss: 5.756\n",
      "Epoch 9 Loss: 2.3025853633880615\n",
      "[11,  1000] loss: 1.151\n",
      "[11,  2000] loss: 2.303\n",
      "[11,  3000] loss: 3.454\n",
      "[11,  4000] loss: 4.605\n",
      "[11,  5000] loss: 5.756\n",
      "Epoch 10 Loss: 2.3025853633880615\n",
      "[12,  1000] loss: 1.151\n",
      "[12,  2000] loss: 2.303\n",
      "[12,  3000] loss: 3.454\n",
      "[12,  4000] loss: 4.605\n",
      "[12,  5000] loss: 5.756\n",
      "Epoch 11 Loss: 2.3025853633880615\n",
      "[13,  1000] loss: 1.151\n",
      "[13,  2000] loss: 2.303\n",
      "[13,  3000] loss: 3.454\n",
      "[13,  4000] loss: 4.605\n",
      "[13,  5000] loss: 5.756\n",
      "Epoch 12 Loss: 2.3025853633880615\n",
      "[14,  1000] loss: 1.151\n",
      "[14,  2000] loss: 2.303\n",
      "[14,  3000] loss: 3.454\n",
      "[14,  4000] loss: 4.605\n",
      "[14,  5000] loss: 5.756\n",
      "Epoch 13 Loss: 2.3025853633880615\n",
      "[15,  1000] loss: 1.151\n",
      "[15,  2000] loss: 2.303\n",
      "[15,  3000] loss: 3.454\n",
      "[15,  4000] loss: 4.605\n",
      "[15,  5000] loss: 5.756\n",
      "Epoch 14 Loss: 2.3025853633880615\n",
      "[16,  1000] loss: 1.151\n",
      "[16,  2000] loss: 2.303\n",
      "[16,  3000] loss: 3.454\n",
      "[16,  4000] loss: 4.605\n",
      "[16,  5000] loss: 5.756\n",
      "Epoch 15 Loss: 2.3025853633880615\n",
      "[17,  1000] loss: 1.151\n",
      "[17,  2000] loss: 2.303\n",
      "[17,  3000] loss: 3.454\n",
      "[17,  4000] loss: 4.605\n",
      "[17,  5000] loss: 5.756\n",
      "Epoch 16 Loss: 2.3025853633880615\n",
      "[18,  1000] loss: 1.151\n",
      "[18,  2000] loss: 2.303\n",
      "[18,  3000] loss: 3.454\n",
      "[18,  4000] loss: 4.605\n",
      "[18,  5000] loss: 5.756\n",
      "Epoch 17 Loss: 2.3025853633880615\n",
      "[19,  1000] loss: 1.151\n",
      "[19,  2000] loss: 2.303\n",
      "[19,  3000] loss: 3.454\n",
      "[19,  4000] loss: 4.605\n",
      "[19,  5000] loss: 5.756\n",
      "Epoch 18 Loss: 2.3025853633880615\n",
      "[20,  1000] loss: 1.151\n",
      "[20,  2000] loss: 2.303\n",
      "[20,  3000] loss: 3.454\n",
      "[20,  4000] loss: 4.605\n",
      "[20,  5000] loss: 5.756\n",
      "Epoch 19 Loss: 2.3025853633880615\n"
     ]
    }
   ],
   "source": [
    "net = DeeperNetwork()\n",
    "net.train()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.01)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data,label) in enumerate(dataSet):\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(data)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "    print(f\"Epoch {epoch} Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3025853633880615\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "running_loss = 0\n",
    "for i, (img, label) in enumerate(testSet):\n",
    "    pred = net(img)\n",
    "    loss = criterion(pred,label)\n",
    "    running_loss += loss.item()\n",
    "print(f\"Test Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss: 2.3025853633880615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: (3/15 pts)\n",
    "\n",
    "Implement Linear Classifier for CIFAR-10 dataset based on code from Part 2. (i.e. treat it as neural networks but use linear layers)\n",
    "\n",
    "Report the accuracy on test set for both with and without bias term:\n",
    "\n",
    "i.e. $y = Wx$ or $y = Wx + b$\n",
    "\n",
    "And both single linear layer and multiple linear layers.\n",
    "\n",
    "Report and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLinearNetwork(nn.Module):\n",
    "    def __init__(self,numLayers=10):\n",
    "        super(SingleLinearNetwork,self).__init__()\n",
    "        convLayers = []\n",
    "        for i in range(numLayers):\n",
    "            convLayers += convBlock(3,3)\n",
    "        self.convs= nn.Sequential(*convLayers)\n",
    "        self.size = 3*32*32 # Size of single image\n",
    "        self.linear = nn.Sequential(nn.Linear(self.size,10),\n",
    "                                    nn.ReLU(),\n",
    "                                   )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,self.size)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.172\n",
      "[1,  2000] loss: 2.324\n",
      "[1,  3000] loss: 3.475\n",
      "[1,  4000] loss: 4.626\n",
      "[1,  5000] loss: 5.778\n",
      "Epoch 0 Loss: 2.3110009608745576\n",
      "[2,  1000] loss: 1.151\n",
      "[2,  2000] loss: 2.303\n",
      "[2,  3000] loss: 3.454\n",
      "[2,  4000] loss: 4.605\n",
      "[2,  5000] loss: 5.756\n",
      "Epoch 1 Loss: 2.3025853633880615\n",
      "[3,  1000] loss: 1.151\n",
      "[3,  2000] loss: 2.303\n",
      "[3,  3000] loss: 3.454\n",
      "[3,  4000] loss: 4.605\n",
      "[3,  5000] loss: 5.756\n",
      "Epoch 2 Loss: 2.3025853633880615\n",
      "[4,  1000] loss: 1.151\n",
      "[4,  2000] loss: 2.303\n",
      "[4,  3000] loss: 3.454\n",
      "[4,  4000] loss: 4.605\n",
      "[4,  5000] loss: 5.756\n",
      "Epoch 3 Loss: 2.3025853633880615\n",
      "[5,  1000] loss: 1.151\n",
      "[5,  2000] loss: 2.303\n",
      "[5,  3000] loss: 3.454\n",
      "[5,  4000] loss: 4.605\n",
      "[5,  5000] loss: 5.756\n",
      "Epoch 4 Loss: 2.3025853633880615\n",
      "[6,  1000] loss: 1.151\n",
      "[6,  2000] loss: 2.303\n",
      "[6,  3000] loss: 3.454\n",
      "[6,  4000] loss: 4.605\n",
      "[6,  5000] loss: 5.756\n",
      "Epoch 5 Loss: 2.3025853633880615\n",
      "[7,  1000] loss: 1.151\n",
      "[7,  2000] loss: 2.303\n",
      "[7,  3000] loss: 3.454\n",
      "[7,  4000] loss: 4.605\n",
      "[7,  5000] loss: 5.756\n",
      "Epoch 6 Loss: 2.3025853633880615\n",
      "[8,  1000] loss: 1.151\n",
      "[8,  2000] loss: 2.303\n",
      "[8,  3000] loss: 3.454\n",
      "[8,  4000] loss: 4.605\n",
      "[8,  5000] loss: 5.756\n",
      "Epoch 7 Loss: 2.3025853633880615\n",
      "[9,  1000] loss: 1.151\n",
      "[9,  2000] loss: 2.303\n",
      "[9,  3000] loss: 3.454\n",
      "[9,  4000] loss: 4.605\n",
      "[9,  5000] loss: 5.756\n",
      "Epoch 8 Loss: 2.3025853633880615\n",
      "[10,  1000] loss: 1.151\n",
      "[10,  2000] loss: 2.303\n",
      "[10,  3000] loss: 3.454\n",
      "[10,  4000] loss: 4.605\n",
      "[10,  5000] loss: 5.756\n",
      "Epoch 9 Loss: 2.3025853633880615\n",
      "[11,  1000] loss: 1.151\n",
      "[11,  2000] loss: 2.303\n",
      "[11,  3000] loss: 3.454\n",
      "[11,  4000] loss: 4.605\n",
      "[11,  5000] loss: 5.756\n",
      "Epoch 10 Loss: 2.3025853633880615\n",
      "[12,  1000] loss: 1.151\n",
      "[12,  2000] loss: 2.303\n",
      "[12,  3000] loss: 3.454\n",
      "[12,  4000] loss: 4.605\n",
      "[12,  5000] loss: 5.756\n",
      "Epoch 11 Loss: 2.3025853633880615\n",
      "[13,  1000] loss: 1.151\n",
      "[13,  2000] loss: 2.303\n",
      "[13,  3000] loss: 3.454\n",
      "[13,  4000] loss: 4.605\n",
      "[13,  5000] loss: 5.756\n",
      "Epoch 12 Loss: 2.3025853633880615\n",
      "[14,  1000] loss: 1.151\n",
      "[14,  2000] loss: 2.303\n",
      "[14,  3000] loss: 3.454\n",
      "[14,  4000] loss: 4.605\n",
      "[14,  5000] loss: 5.756\n",
      "Epoch 13 Loss: 2.3025853633880615\n",
      "[15,  1000] loss: 1.151\n",
      "[15,  2000] loss: 2.303\n",
      "[15,  3000] loss: 3.454\n",
      "[15,  4000] loss: 4.605\n",
      "[15,  5000] loss: 5.756\n",
      "Epoch 14 Loss: 2.3025853633880615\n",
      "[16,  1000] loss: 1.151\n",
      "[16,  2000] loss: 2.303\n",
      "[16,  3000] loss: 3.454\n",
      "[16,  4000] loss: 4.605\n",
      "[16,  5000] loss: 5.756\n",
      "Epoch 15 Loss: 2.3025853633880615\n",
      "[17,  1000] loss: 1.151\n",
      "[17,  2000] loss: 2.303\n",
      "[17,  3000] loss: 3.454\n",
      "[17,  4000] loss: 4.605\n",
      "[17,  5000] loss: 5.756\n",
      "Epoch 16 Loss: 2.3025853633880615\n",
      "[18,  1000] loss: 1.151\n",
      "[18,  2000] loss: 2.303\n",
      "[18,  3000] loss: 3.454\n",
      "[18,  4000] loss: 4.605\n",
      "[18,  5000] loss: 5.756\n",
      "Epoch 17 Loss: 2.3025853633880615\n",
      "[19,  1000] loss: 1.151\n",
      "[19,  2000] loss: 2.303\n",
      "[19,  3000] loss: 3.454\n",
      "[19,  4000] loss: 4.605\n",
      "[19,  5000] loss: 5.756\n",
      "Epoch 18 Loss: 2.3025853633880615\n",
      "[20,  1000] loss: 1.151\n",
      "[20,  2000] loss: 2.303\n",
      "[20,  3000] loss: 3.454\n",
      "[20,  4000] loss: 4.605\n",
      "[20,  5000] loss: 5.756\n",
      "Epoch 19 Loss: 2.3025853633880615\n"
     ]
    }
   ],
   "source": [
    "net = SingleLinearNetwork()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.01)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data,label) in enumerate(dataSet):\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(data)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "    print(f\"Epoch {epoch} Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3025853633880615\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "running_loss = 0\n",
    "for i, (img, label) in enumerate(testSet):\n",
    "    pred = net(img)\n",
    "    loss = criterion(pred,label)\n",
    "    running_loss += loss.item()\n",
    "print(f\"Test Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss: 2.3025853633880615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self,numLayers=10):\n",
    "        super(LinearNetwork,self).__init__()\n",
    "        convLayers = []\n",
    "        for i in range(numLayers):\n",
    "            convLayers += convBlock(3,3)\n",
    "        self.convs= nn.Sequential(*convLayers)\n",
    "        self.size = 3*32*32 # Size of single image\n",
    "        self.linear = nn.Sequential(nn.Linear(self.size,1000),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(1000,500),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(500,10),\n",
    "                                   )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,self.size)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.243\n",
      "[1,  2000] loss: 2.395\n",
      "[1,  3000] loss: 3.547\n",
      "[1,  4000] loss: 4.700\n",
      "[1,  5000] loss: 5.852\n",
      "Epoch 0 Loss: 2.340956567239761\n",
      "[2,  1000] loss: 1.153\n",
      "[2,  2000] loss: 2.305\n",
      "[2,  3000] loss: 3.458\n",
      "[2,  4000] loss: 4.610\n",
      "[2,  5000] loss: 5.763\n",
      "Epoch 1 Loss: 2.3051849474430086\n",
      "[3,  1000] loss: 1.153\n",
      "[3,  2000] loss: 2.305\n",
      "[3,  3000] loss: 3.458\n",
      "[3,  4000] loss: 4.610\n",
      "[3,  5000] loss: 5.763\n",
      "Epoch 2 Loss: 2.3051904609680176\n",
      "[4,  1000] loss: 1.152\n",
      "[4,  2000] loss: 2.305\n",
      "[4,  3000] loss: 3.458\n",
      "[4,  4000] loss: 4.611\n",
      "[4,  5000] loss: 5.763\n",
      "Epoch 3 Loss: 2.3053984189987182\n",
      "[5,  1000] loss: 1.153\n",
      "[5,  2000] loss: 2.305\n",
      "[5,  3000] loss: 3.458\n",
      "[5,  4000] loss: 4.610\n",
      "[5,  5000] loss: 5.763\n",
      "Epoch 4 Loss: 2.305188440847397\n",
      "[6,  1000] loss: 1.152\n",
      "[6,  2000] loss: 2.305\n",
      "[6,  3000] loss: 3.458\n",
      "[6,  4000] loss: 4.610\n",
      "[6,  5000] loss: 5.762\n",
      "Epoch 5 Loss: 2.3049027447223662\n",
      "[7,  1000] loss: 1.152\n",
      "[7,  2000] loss: 2.305\n",
      "[7,  3000] loss: 3.457\n",
      "[7,  4000] loss: 4.610\n",
      "[7,  5000] loss: 5.763\n",
      "Epoch 6 Loss: 2.305002449464798\n",
      "[8,  1000] loss: 1.153\n",
      "[8,  2000] loss: 2.305\n",
      "[8,  3000] loss: 3.458\n",
      "[8,  4000] loss: 4.611\n",
      "[8,  5000] loss: 5.764\n",
      "Epoch 7 Loss: 2.3054839893341064\n",
      "[9,  1000] loss: 1.153\n",
      "[9,  2000] loss: 2.306\n",
      "[9,  3000] loss: 3.459\n",
      "[9,  4000] loss: 4.612\n",
      "[9,  5000] loss: 5.764\n",
      "Epoch 8 Loss: 2.305609016418457\n",
      "[10,  1000] loss: 1.153\n",
      "[10,  2000] loss: 2.305\n",
      "[10,  3000] loss: 3.458\n",
      "[10,  4000] loss: 4.611\n",
      "[10,  5000] loss: 5.763\n",
      "Epoch 9 Loss: 2.3051748784542085\n",
      "[11,  1000] loss: 1.153\n",
      "[11,  2000] loss: 2.305\n",
      "[11,  3000] loss: 3.458\n",
      "[11,  4000] loss: 4.611\n",
      "[11,  5000] loss: 5.763\n",
      "Epoch 10 Loss: 2.305236393404007\n",
      "[12,  1000] loss: 1.152\n",
      "[12,  2000] loss: 2.305\n",
      "[12,  3000] loss: 3.457\n",
      "[12,  4000] loss: 4.610\n",
      "[12,  5000] loss: 5.762\n",
      "Epoch 11 Loss: 2.304945050096512\n",
      "[13,  1000] loss: 1.152\n",
      "[13,  2000] loss: 2.305\n",
      "[13,  3000] loss: 3.457\n",
      "[13,  4000] loss: 4.609\n",
      "[13,  5000] loss: 5.762\n",
      "Epoch 12 Loss: 2.3048264004707337\n",
      "[14,  1000] loss: 1.152\n",
      "[14,  2000] loss: 2.305\n",
      "[14,  3000] loss: 3.457\n",
      "[14,  4000] loss: 4.609\n",
      "[14,  5000] loss: 5.762\n",
      "Epoch 13 Loss: 2.3046434302806853\n",
      "[15,  1000] loss: 1.152\n",
      "[15,  2000] loss: 2.305\n",
      "[15,  3000] loss: 3.458\n",
      "[15,  4000] loss: 4.610\n",
      "[15,  5000] loss: 5.763\n",
      "Epoch 14 Loss: 2.305074845075607\n",
      "[16,  1000] loss: 1.153\n",
      "[16,  2000] loss: 2.305\n",
      "[16,  3000] loss: 3.458\n",
      "[16,  4000] loss: 4.610\n",
      "[16,  5000] loss: 5.763\n",
      "Epoch 15 Loss: 2.305157451725006\n",
      "[17,  1000] loss: 1.153\n",
      "[17,  2000] loss: 2.305\n",
      "[17,  3000] loss: 3.458\n",
      "[17,  4000] loss: 4.611\n",
      "[17,  5000] loss: 5.763\n",
      "Epoch 16 Loss: 2.305144755601883\n",
      "[18,  1000] loss: 1.152\n",
      "[18,  2000] loss: 2.305\n",
      "[18,  3000] loss: 3.457\n",
      "[18,  4000] loss: 4.610\n",
      "[18,  5000] loss: 5.762\n",
      "Epoch 17 Loss: 2.304904285669327\n",
      "[19,  1000] loss: 1.152\n",
      "[19,  2000] loss: 2.304\n",
      "[19,  3000] loss: 3.457\n",
      "[19,  4000] loss: 4.609\n",
      "[19,  5000] loss: 5.762\n",
      "Epoch 18 Loss: 2.3048208953380587\n",
      "[20,  1000] loss: 1.152\n",
      "[20,  2000] loss: 2.305\n",
      "[20,  3000] loss: 3.457\n",
      "[20,  4000] loss: 4.610\n",
      "[20,  5000] loss: 5.762\n",
      "Epoch 19 Loss: 2.304853692817688\n"
     ]
    }
   ],
   "source": [
    "net = LinearNetwork()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.01)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data,label) in enumerate(dataSet):\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(data)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "    print(f\"Epoch {epoch} Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3046057651042937\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "running_loss = 0\n",
    "for i, (img, label) in enumerate(testSet):\n",
    "    pred = net(img)\n",
    "    loss = criterion(pred,label)\n",
    "    running_loss += loss.item()\n",
    "print(f\"Test Loss: {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss: 2.3046057651042937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Deeper doesn't really mean better. There's more going into networks that make them better besides depth. Things like pooling layers, batch norms, and dropout make a big difference.\n",
    "\n",
    "\n",
    "I ran short epochs because my home computer can't handle much computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits: (5 pts, pick either one)\n",
    "\n",
    "(a) Fine-tune a ResNet-50 model pre-trained on ImageNet for classifying the Caltech-UCSD Birds dataset (Links to an external site.). \n",
    "\n",
    "Post (Links to an external site.) on how to fine-tune models in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/birds/200\\\\.Common_Yellowthroat/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-43fe3ab17af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../datasets/birds/200\\.Common_Yellowthroat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdataloaders_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-43fe3ab17af4>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../datasets/birds/200\\.Common_Yellowthroat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdataloaders_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    204\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     92\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     93\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/birds/200\\\\.Common_Yellowthroat/train'"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(128),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "data_dir = \"../datasets/birds/200.Common_Yellowthroat\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
